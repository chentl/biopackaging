{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "86c4f81dce73b0c4"
  },
  {
   "cell_type": "code",
   "id": "21e4721d-0aea-4f75-817c-841b5497cdb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T04:36:35.588858Z",
     "start_time": "2025-05-17T04:36:34.493966Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import lz4.frame\n",
    "import dill\n",
    "import json\n",
    "from sklearn.utils import Bunch\n",
    "import torch\n",
    "from matal.utils.cache import RobustJSONEncoder\n",
    "from matal.settings import DATA_DIR\n",
    "from matal.ann_model import X_COLS, TARGET_Y_COLS, TARGET_Y_SCALES"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build Training Dataset with Data Augmentation",
   "id": "f2b8d781e2d6edc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T06:20:44.752989Z",
     "start_time": "2025-03-14T06:08:01.677152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for n_batch in [2**10, 2**20, ]:\n",
    "    batch_size = 32\n",
    "    data_tag = '2503A'\n",
    "    data_mix_rate = {\n",
    "        'grade': 19,\n",
    "        'optical': 4,\n",
    "        'tensile': 5,\n",
    "        'fire': 2,\n",
    "    }\n",
    "    mix_rate_tag = ''.join([f'{k[0]}{v}' for k, v in data_mix_rate.items()])\n",
    "    \n",
    "    data_name = f'{data_tag}_B{batch_size}_N{hex(n_batch)[2:]}_{mix_rate_tag}'\n",
    "    print(data_name)\n",
    "    \n",
    "    targets = list(data_mix_rate.keys())\n",
    "    target_weights = np.array([data_mix_rate[k] for k in targets])\n",
    "    target_weights = target_weights / target_weights.sum()\n",
    "    target_dfs = {\n",
    "        target: pd.read_csv(DATA_DIR / 'weighted' / f'{target}.csv') for target in targets\n",
    "    }\n",
    "    train_dfs = {\n",
    "        target: df[~df.SampleID.str.startswith('aTS-T')].reset_index(drop=True) for target, df in target_dfs.items()\n",
    "    }\n",
    "    test_dfs = {\n",
    "        target: df[df.SampleID.str.startswith('aTS-T')].reset_index(drop=True) for target, df in target_dfs.items()\n",
    "    }\n",
    "    \n",
    "    print(\n",
    "        (target_weights[0] * batch_size * n_batch) / len(target_dfs['grade']), \n",
    "        (target_weights[1] * batch_size * n_batch) / len(target_dfs['optical']),\n",
    "        (target_weights[2] * batch_size * n_batch) / len(target_dfs['tensile']),\n",
    "        (target_weights[3] * batch_size * n_batch) / len(target_dfs['fire'])\n",
    "    )\n",
    "    \n",
    "    batch_ti_arr = []\n",
    "    batch_X_arr = []\n",
    "    batch_y_arr = []\n",
    "    rng = np.random.default_rng(0)\n",
    "    batch_targets = rng.choice(np.arange(len(targets), dtype=np.int8), size=n_batch, p=target_weights)\n",
    "    \n",
    "    for ti in tqdm(batch_targets):\n",
    "        target = targets[ti]\n",
    "        \n",
    "        train_df = train_dfs[target]\n",
    "        target_y_cols = TARGET_Y_COLS[target]\n",
    "        target_y_std_cols = [f'{c}_STD' for c in target_y_cols]\n",
    "        \n",
    "        batch_df = train_df.sample(\n",
    "            n=batch_size, replace=True, random_state=rng, ignore_index=True, weights=train_df['Weight'])\n",
    "        \n",
    "        # Data Augmentation: add noise based on experimental standard deviation\n",
    "        batch_df[target_y_cols] += rng.normal(loc=0.0, scale=batch_df[target_y_std_cols])\n",
    "    \n",
    "        batch_ti_arr.append(ti)\n",
    "        batch_X_arr.append(batch_df[X_COLS].values.astype(np.float16))\n",
    "        batch_y_arr.append(batch_df[TARGET_Y_COLS[target]].values / np.array(TARGET_Y_SCALES[target]).astype(np.float16))\n",
    "    \n",
    "    with lz4.frame.open(DATA_DIR / 'batch_cache' / f'{data_name}.batch.pk.lz4', 'wb') as f:\n",
    "        dill.dump(dict(batch_ti_arr=np.array(batch_ti_arr).astype(np.int8), \n",
    "                       batch_X_arr=batch_X_arr,\n",
    "                       batch_y_arr=batch_y_arr), f)\n",
    "    \n",
    "    config = Bunch()\n",
    "    config.X_COLS = X_COLS\n",
    "    config.TARGET_Y_COLS = TARGET_Y_COLS\n",
    "    config.TARGET_Y_SCALES = TARGET_Y_SCALES\n",
    "    config.target_weights = target_weights\n",
    "    config.batch_size = batch_size\n",
    "    config.n_batch = n_batch\n",
    "    \n",
    "    with open(DATA_DIR / 'batch_cache' / f'{data_name}.config.json', 'w') as f:\n",
    "        json.dump(config, f, indent=2, cls=RobustJSONEncoder)\n",
    "    \n",
    "    all_dataset = {}\n",
    "    for target in targets:\n",
    "        train_df = train_dfs[target]\n",
    "        if len(train_df) == 0:\n",
    "            continue\n",
    "        all_dataset[target] = Bunch(\n",
    "            scale=TARGET_Y_SCALES[target],\n",
    "            train_X=torch.as_tensor(train_dfs[target][X_COLS].values),\n",
    "            train_y=torch.as_tensor(train_dfs[target][TARGET_Y_COLS[target]].values / np.array(TARGET_Y_SCALES[target])),\n",
    "    \n",
    "            train_X_df=train_dfs[target][X_COLS],\n",
    "            train_y_df=train_dfs[target][TARGET_Y_COLS[target]],\n",
    "        )\n",
    "    \n",
    "        test_df = test_dfs[target]\n",
    "        if len(test_df) == 0:\n",
    "            continue\n",
    "        all_dataset[target].test_X = torch.as_tensor(test_dfs[target][X_COLS].values)\n",
    "        all_dataset[target].test_y = torch.as_tensor(test_dfs[target][TARGET_Y_COLS[target]].values / np.array(TARGET_Y_SCALES[target]))\n",
    "    \n",
    "        all_dataset[target].test_X_df=test_dfs[target][X_COLS]\n",
    "        all_dataset[target].test_y_df=test_dfs[target][TARGET_Y_COLS[target]]\n",
    "    \n",
    "    with lz4.frame.open(DATA_DIR / 'batch_cache' / f'{data_name}.all_data.pk.lz4', 'wb') as f:\n",
    "        dill.dump(all_dataset, f)\n",
    "    \n"
   ],
   "id": "81ca56449f84dab6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2503A_B32_N400_g19o4t5f2\n",
      "6.5467087276550995 12.307230046948357 15.648519579751671 8.0610086100861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56077c6a23e941e2a5658e698be60a42"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2503A_B32_N100000_g19o4t5f2\n",
      "6703.829737118822 12602.603568075117 16024.084049665711 8254.472816728166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1048576 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98ca8f9b6c71499b9572ba9b1907ec5d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d83fc54db754b49d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
