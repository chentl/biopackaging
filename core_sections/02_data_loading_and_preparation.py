# ================================
# DATA LOADING AND PREPARATION
# ================================
"""
Data loading, cleaning, and preparation utilities for the ensemble neural network pipeline.
This module handles all data preprocessing operations before model training.
"""
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# Load and prepare dataset
df = pd.read_csv('data/ann_demo.csv')
print(f"ğŸ“ Loaded dataset shape: {df.shape}")

# Clean data by removing rows with missing targets
# Only use target columns that exist in the dataset
all_target_cols = [col for cols in TARGET_Y_COLS.values() for col in cols]
existing_target_cols = [col for col in all_target_cols if col in df.columns]
missing_cols = [col for col in all_target_cols if col not in df.columns]

if missing_cols:
    print(f"âš ï¸  Note: Columns not in dataset (skipping): {missing_cols}")

df_clean = df.dropna(subset=existing_target_cols)

print(f"ğŸ§¹ After removing missing targets: {df_clean.shape}")
print(f"ğŸ“ˆ Data retention: {len(df_clean)/len(df)*100:.1f}%")

# Display basic information
print(f"\nğŸ“‹ Dataset Summary:")
print(f"   Total samples: {len(df_clean)}")
print(f"   Input features: {len(X_COLS)}")
print(f"   Target variables: {len(all_target_cols)}")
print(f"   Missing values in targets: {df_clean[all_target_cols].isnull().sum().sum()}")

# Display target statistics (only for existing columns)
print(f"\nğŸ“Š Target Variable Ranges:")
for task, cols in TARGET_Y_COLS.items():
    existing_cols = [col for col in cols if col in df_clean.columns]
    if existing_cols:
        print(f"\n{task.upper()} Targets:")
        for col in existing_cols:
            min_val = df_clean[col].min()
            max_val = df_clean[col].max()
            mean_val = df_clean[col].mean()
            print(f"  {col}: [{min_val:.2f}, {max_val:.2f}] (mean: {mean_val:.2f})")
    else:
        print(f"\n{task.upper()}: âš ï¸  No columns found in dataset")

def validate_data_preparation():
    """Validate data preparation and provide comprehensive summary."""
    print("\nâœ… [02] Data Loading and Preparation Module Loaded")
    print(f"ğŸ—‚ï¸  Dataset Processing Summary:")
    print(f"   ğŸ“Š Original dataset: {df.shape[0]} samples")
    print(f"   ğŸ§¹ After cleaning: {df_clean.shape[0]} samples")
    print(f"   ğŸ“ˆ Data retention: {len(df_clean)/len(df)*100:.1f}%")
    print(f"   ğŸ”— Input features: {len(X_COLS)}")
    print(f"   ğŸ¯ Target variables: {len(all_target_cols)}")
    
    print(f"ğŸ“Š Deep Dive - Target Statistics:")
    for task, cols in TARGET_Y_COLS.items():
        existing_cols = [col for col in cols if col in df_clean.columns]
        if existing_cols:
            print(f"\n{task.upper()} Properties:")
            for col in existing_cols:
                min_val = df_clean[col].min()
                max_val = df_clean[col].max()
                mean_val = df_clean[col].mean()
                std_val = df_clean[col].std()
                print(f"  {col}: [{min_val:.3f}, {max_val:.3f}] Î¼={mean_val:.3f}, Ïƒ={std_val:.3f}")
    
    return df_clean

# For backward compatibility with existing variable name
df = df_clean

# Auto-validate on import
if __name__ == "__main__" or __name__ == "__file__":
    validate_data_preparation()
