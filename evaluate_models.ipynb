{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import lz4.frame\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "\n",
    "# Add the matal module to path\n",
    "sys.path.append(str(Path.cwd() / 'matal'))\n",
    "\n",
    "from matal.ann_model.config import ModelConfig\n",
    "from matal.ann_model.builder import build_model\n",
    "from matal.ann_model.settings import X_COLS, TARGET_Y_COLS, TARGET_Y_SCALES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (3170, 31), ANN Demo: (342, 42)\n",
      "Removed 342 rows. Final: (2828, 31)\n"
     ]
    }
   ],
   "source": [
    "# Load datasets and remove overlapping SampleIDs\n",
    "grade1_df = pd.read_csv('data/grade 1 (2).csv')\n",
    "ann_demo_df = pd.read_csv('data/ann_demo.csv')\n",
    "\n",
    "print(f\"Original: {grade1_df.shape}, ANN Demo: {ann_demo_df.shape}\")\n",
    "\n",
    "# Remove overlapping SampleIDs\n",
    "overlapping = set(grade1_df['SampleID']).intersection(set(ann_demo_df['SampleID']))\n",
    "grade1_df = grade1_df[~grade1_df['SampleID'].isin(ann_demo_df['SampleID'])]\n",
    "\n",
    "print(f\"Removed {len(overlapping)} rows. Final: {grade1_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 2648 samples\n",
      "Test set: 180 samples\n",
      "Total: 2828 samples\n"
     ]
    }
   ],
   "source": [
    "# Split grade1_df into train and test based on SampleID prefix\n",
    "test_mask = grade1_df['SampleID'].str.startswith('aTS-T')\n",
    "test_df = grade1_df[test_mask].copy()\n",
    "train_df = grade1_df[~test_mask].copy()\n",
    "\n",
    "print(f\"Train set: {train_df.shape[0]} samples\")\n",
    "print(f\"Test set: {test_df.shape[0]} samples\")\n",
    "print(f\"Total: {train_df.shape[0] + test_df.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure directory: model/v3hp\n",
      "Results directory: evaluation_results\n"
     ]
    }
   ],
   "source": [
    "# Set up paths\n",
    "MODEL_STRUCTURE_DIR = Path('model/v3hp/')\n",
    "RESULTS_DIR = Path('./evaluation_results')\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Model structure directory: {MODEL_STRUCTURE_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available model files:\n",
      "  - model/v3hp/pt_db.v3hp.RE5C/pt_db.v3hp.RE5C.model.pt.lz4\n",
      "  - model/v3hp/pt_db.v3hp.2HNP/pt_db.v3hp.2HNP.model.pt.lz4\n",
      "  - model/v3hp/pt_db.v3hp.CPUT/pt_db.v3hp.CPUT.model.pt.lz4\n",
      "  - model/v3hp/pt_db.v3hp.VPLV/pt_db.v3hp.VPLV.model.pt.lz4\n",
      "  - model/v3hp/pt_db.v3hp.PFFZ/pt_db.v3hp.PFFZ.model.pt.lz4\n",
      "\n",
      "Model tags: ['pt_db.v3hp.RE5C', 'pt_db.v3hp.2HNP', 'pt_db.v3hp.CPUT', 'pt_db.v3hp.VPLV', 'pt_db.v3hp.PFFZ']\n"
     ]
    }
   ],
   "source": [
    "# List available models in model_structure directory (search in subdirectories)\n",
    "model_files = list(MODEL_STRUCTURE_DIR.glob('**/*.model.pt.lz4'))\n",
    "print(\"Available model files:\")\n",
    "for model_file in model_files:\n",
    "    print(f\"  - {model_file}\")\n",
    "\n",
    "# Extract model tags\n",
    "model_tags = []\n",
    "for model_file in model_files:\n",
    "    # Extract model tag from filename (e.g., pt_db.v3hp.2HNP.model.pt.lz4 -> pt_db.v3hp.2HNP)\n",
    "    tag = model_file.name.replace('.model.pt.lz4', '')\n",
    "    model_tags.append(tag)\n",
    "\n",
    "print(f\"\\nModel tags: {model_tags}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_model_txt_to_params(model_txt_path):\n",
    "    \"\"\"\n",
    "    Parse a .model.txt file to extract model parameters.\n",
    "    \n",
    "    Args:\n",
    "        model_txt_path: Path to the .model.txt file\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing model parameters in the expected format\n",
    "    \"\"\"\n",
    "    with open(model_txt_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Initialize parameters dictionary\n",
    "    params = {\n",
    "        'model_params': {},\n",
    "        'data_config': {\n",
    "            'X_COLS': X_COLS,\n",
    "            'TARGET_Y_COLS': TARGET_Y_COLS,\n",
    "            'TARGET_Y_SCALES': TARGET_Y_SCALES\n",
    "        }\n",
    "    }\n",
    "    print(\"Here\")\n",
    "    # Parse encoder architecture\n",
    "    encoder_match = re.search(r'\\(encoder\\): ANNModel\\((.*?)\\)', content, re.DOTALL)\n",
    "    if encoder_match:\n",
    "        encoder_content = encoder_match.group(1)\n",
    "        encoder_layers = parse_ann_model_layers(encoder_content)\n",
    "        \n",
    "        # Extract encoder parameters\n",
    "        params['model_params']['encoder__hidden_layers'] = len(encoder_layers) - 1  # -1 for input layer\n",
    "        params['model_params']['encoder__n_output'] = encoder_layers[-1]['out_features']\n",
    "        params['model_params']['encoder__hidden_base'] = encoder_layers[1]['out_features']  # First hidden layer size\n",
    "        params['model_params']['encoder__hidden_scale'] = 'linear'  # Default assumption\n",
    "        params['model_params']['encoder__act_hidden'] = 'elu'  # From the file\n",
    "        params['model_params']['encoder__act_output'] = 'identity'  # From the file\n",
    "        params['model_params']['encoder__weight_init'] = 'kaiming_uniform_'\n",
    "        params['model_params']['encoder__input_dropout'] = 0.0\n",
    "        params['model_params']['encoder__output_dropout'] = 0.0\n",
    "        params['model_params']['encoder__hidden_dropout'] = 0.0\n",
    "    \n",
    "    # Parse task-specific models (grade, optical, tensile, fire)\n",
    "    for task in ['grade', 'optical', 'tensile', 'fire']:\n",
    "        task_match = re.search(rf'\\({task}\\): ANNModel\\((.*?)\\)', content, re.DOTALL)\n",
    "        if task_match:\n",
    "            task_content = task_match.group(1)\n",
    "            task_layers = parse_ann_model_layers(task_content)\n",
    "            \n",
    "            # Extract task parameters\n",
    "            params['model_params'][f'{task}__hidden_layers'] = len(task_layers) - 1\n",
    "            params['model_params'][f'{task}__n_output'] = task_layers[-1]['out_features']\n",
    "            params['model_params'][f'{task}__hidden_base'] = task_layers[1]['out_features'] if len(task_layers) > 1 else task_layers[0]['out_features']\n",
    "            params['model_params'][f'{task}__hidden_scale'] = 'linear'\n",
    "            params['model_params'][f'{task}__act_hidden'] = 'elu'\n",
    "            \n",
    "            # Determine output activation based on the last layer\n",
    "            last_activation = task_layers[-1]['activation']\n",
    "            if last_activation == 'Sigmoid':\n",
    "                params['model_params'][f'{task}__act_output'] = 'sigmoid'\n",
    "            elif last_activation == 'ReLU':\n",
    "                params['model_params'][f'{task}__act_output'] = 'relu'\n",
    "            else:\n",
    "                params['model_params'][f'{task}__act_output'] = 'identity'\n",
    "            \n",
    "            params['model_params'][f'{task}__weight_init'] = 'kaiming_uniform_'\n",
    "            params['model_params'][f'{task}__input_dropout'] = 0.0\n",
    "            params['model_params'][f'{task}__output_dropout'] = 0.0\n",
    "            params['model_params'][f'{task}__hidden_dropout'] = 0.0\n",
    "    \n",
    "    # Add general parameters\n",
    "    params['model_params']['random_seed'] = 0\n",
    "    \n",
    "    return params\n",
    "\n",
    "def parse_ann_model_layers(content):\n",
    "    \"\"\"\n",
    "    Parse ANNModel layers from the model text content.\n",
    "    \n",
    "    Args:\n",
    "        content: The layers section content\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing layer information\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    \n",
    "    # Find all Linear layers\n",
    "    linear_matches = re.finditer(r'\\(fc(\\d+)\\): Linear\\(in_features=(\\d+), out_features=(\\d+), bias=True\\)', content)\n",
    "    for match in linear_matches:\n",
    "        layer_num = int(match.group(1))\n",
    "        in_features = int(match.group(2))\n",
    "        out_features = int(match.group(3))\n",
    "        \n",
    "        # Find corresponding activation\n",
    "        activation_match = re.search(rf'\\(fc{layer_num}_act\\): (\\w+)', content)\n",
    "        activation = activation_match.group(1) if activation_match else 'Identity'\n",
    "        \n",
    "        layers.append({\n",
    "            'layer_num': layer_num,\n",
    "            'in_features': in_features,\n",
    "            'out_features': out_features,\n",
    "            'activation': activation\n",
    "        })\n",
    "    \n",
    "    # Sort by layer number\n",
    "    layers.sort(key=lambda x: x['layer_num'])\n",
    "    \n",
    "    return layers\n",
    "\n",
    "def load_model_from_structure_with_txt(model_tag, model_dir=MODEL_STRUCTURE_DIR):\n",
    "    \"\"\"\n",
    "    Load a model from the model_structure directory using .model.txt files.\n",
    "    \n",
    "    Args:\n",
    "        model_tag: Model tag (e.g., 'pt_db.v3hp.2HNP')\n",
    "        model_dir: Directory containing model files\n",
    "    \n",
    "    Returns:\n",
    "        Loaded model and parameters\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Model files are in subdirectories named after the model tag\n",
    "        # e.g., model/v3hp/pt_db.v3hp.2HNP/pt_db.v3hp.2HNP.param.pk\n",
    "        model_subdir = model_dir / model_tag\n",
    "        \n",
    "        # Try to load parameters from .param.pk first\n",
    "        param_file = model_subdir / f'{model_tag}.param.pk'\n",
    "        if param_file.exists():\n",
    "            with open(param_file, 'rb') as f:\n",
    "                params = pickle.load(f)\n",
    "        else:\n",
    "            # Extract the base model name from the tag (e.g., 'pt_db.v3hp.2HNP' -> 'pt_db.2HNP')\n",
    "            # Split by dots and reconstruct without the version part\n",
    "            tag_parts = model_tag.split('.')\n",
    "            if len(tag_parts) >= 3 and tag_parts[1].startswith('v'):\n",
    "                # Remove version part (e.g., 'v3hp')\n",
    "                base_tag = f\"{tag_parts[0]}.{'.'.join(tag_parts[2:])}\"\n",
    "            else:\n",
    "                base_tag = model_tag\n",
    "            \n",
    "            # Parse from .model.txt file\n",
    "            model_txt_file = model_subdir / f'{base_tag}.model.txt'\n",
    "            if not model_txt_file.exists():\n",
    "                print(f\"Neither parameter file nor model.txt file found for {model_tag}\")\n",
    "                print(f\"Tried: {param_file} and {model_txt_file}\")\n",
    "                return None, None\n",
    "            \n",
    "            print(f\"Parsing model parameters from {model_txt_file}\")\n",
    "            params = parse_model_txt_to_params(model_txt_file)\n",
    "        \n",
    "        print(params)\n",
    "        # Build model\n",
    "        model_config = ModelConfig(**params['model_params'])\n",
    "        model = build_model(model_name=model_tag, **model_config.get_all())\n",
    "        \n",
    "        # Load model weights\n",
    "        model_file = model_subdir / f'{model_tag}.model.pt.lz4'\n",
    "        if not model_file.exists():\n",
    "            print(f\"Model file not found: {model_file}\")\n",
    "            return None, None\n",
    "        \n",
    "        with lz4.frame.open(model_file, 'rb') as f:\n",
    "            model.load_state_dict(torch.load(f))\n",
    "        \n",
    "        model.eval()\n",
    "        print(f\"Successfully loaded model: {model_tag}\")\n",
    "        return model, params\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model {model_tag}: {e}\")\n",
    "        return None, None\n",
    "# Test loading one model\n",
    "if model_tags:\n",
    "    print(\"Available model tags:\", model_tags)\n",
    "    print(\"\\nTesting model loading with .model.txt files...\")\n",
    "    \n",
    "    # Test the naming conversion\n",
    "    for tag in model_tags:\n",
    "        tag_parts = tag.split('.')\n",
    "        if len(tag_parts) >= 3 and tag_parts[1].startswith('v'):\n",
    "            base_tag = f\"{tag_parts[0]}.{'.'.join(tag_parts[2:])}\"\n",
    "            print(f\"Model tag: {tag} -> Base tag: {base_tag}\")\n",
    "    \n",
    "    # Load the first model\n",
    "    test_model, test_params = load_model_from_structure_with_txt(model_tags[0])\n",
    "    if test_model is not None:\n",
    "        print(f\"\\nModel architecture:\")\n",
    "        print(test_model)\n",
    "        print(f\"\\nModel parameters:\")\n",
    "        print(test_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate -2k - Feasibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing grade datasets for evaluation...\n",
      "Prepared grade dataset: train (2648, 3), test (180, 3)\n",
      "Grade targets: ['Detachability', 'FlatnessUni', 'Feasibility']\n"
     ]
    }
   ],
   "source": [
    "# Fixed version of prepare_grade_datasets_for_evaluation function\n",
    "def prepare_grade_datasets_for_evaluation_fixed(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Prepare train and test datasets for grade task evaluation.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training dataframe\n",
    "        test_df: Test dataframe\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing prepared grade datasets\n",
    "    \"\"\"\n",
    "    # Extract input features (X_COLS) and ensure float32\n",
    "    X_train = train_df[X_COLS].values.astype(np.float32)\n",
    "    X_test = test_df[X_COLS].values.astype(np.float32)\n",
    "    \n",
    "    # Extract grade target values and ensure float32\n",
    "    grade_cols = TARGET_Y_COLS['grade']\n",
    "    y_train = train_df[grade_cols].values.astype(np.float32)\n",
    "    y_test = test_df[grade_cols].values.astype(np.float32)\n",
    "    \n",
    "    dataset = {\n",
    "        'train_X': torch.tensor(X_train, dtype=torch.float32),\n",
    "        'train_y': torch.tensor(y_train, dtype=torch.float32),\n",
    "        'test_X': torch.tensor(X_test, dtype=torch.float32),\n",
    "        'test_y': torch.tensor(y_test, dtype=torch.float32)\n",
    "    }\n",
    "    \n",
    "    print(f\"Prepared grade dataset: train {y_train.shape}, test {y_test.shape}\")\n",
    "    print(f\"Grade targets: {grade_cols}\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Prepare grade datasets with fixed function\n",
    "print(\"Preparing grade datasets for evaluation...\")\n",
    "grade_dataset = prepare_grade_datasets_for_evaluation_fixed(train_df, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed version of calculate_grade_model_losses function\n",
    "def calculate_grade_model_losses_fixed(model, dataset, model_tag, device='cpu'):\n",
    "    \"\"\"\n",
    "    Calculate grade task losses for a model on train and test datasets.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model\n",
    "        dataset: Dictionary containing train/test datasets for grade task\n",
    "        model_tag: Model identifier\n",
    "        device: Device to run evaluation on\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing loss results\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Ensure model parameters are float32\n",
    "    for param in model.parameters():\n",
    "        param.data = param.data.float()\n",
    "    \n",
    "    # Use BCEWithLogitsLoss for grade task (binary classification)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    results = {'model_tag': model_tag}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Calculate train loss\n",
    "        train_X = dataset['train_X'].to(device).float()\n",
    "        train_y = dataset['train_y'].to(device).float()\n",
    "        train_pred = model.forward(train_X, 'grade')\n",
    "        train_loss = criterion(train_pred, train_y)\n",
    "        results['grade_train_loss'] = train_loss.item()\n",
    "        \n",
    "        # Calculate test loss\n",
    "        test_X = dataset['test_X'].to(device).float()\n",
    "        test_y = dataset['test_y'].to(device).float()\n",
    "        test_pred = model.forward(test_X, 'grade')\n",
    "        test_loss = criterion(test_pred, test_y)\n",
    "        results['grade_test_loss'] = test_loss.item()\n",
    "        \n",
    "        print(f\"{model_tag} - Grade: Train Loss = {train_loss.item():.6f}, Test Loss = {test_loss.item():.6f}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating grade losses for all models...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:00<00:00,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.RE5C\n",
      "pt_db.v3hp.RE5C - Grade: Train Loss = 0.556246, Test Loss = 0.659720\n",
      "Completed evaluation for pt_db.v3hp.RE5C\n",
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.2HNP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:00<00:00,  9.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt_db.v3hp.2HNP - Grade: Train Loss = 0.571103, Test Loss = 0.658976\n",
      "Completed evaluation for pt_db.v3hp.2HNP\n",
      "{'model_params': {'encoder__hidden_layers': 50, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.CPUT\n",
      "pt_db.v3hp.CPUT - Grade: Train Loss = 0.573922, Test Loss = 0.651741\n",
      "Completed evaluation for pt_db.v3hp.CPUT\n",
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.VPLV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  9.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt_db.v3hp.VPLV - Grade: Train Loss = 0.558082, Test Loss = 0.655647\n",
      "Completed evaluation for pt_db.v3hp.VPLV\n",
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 400.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.PFFZ\n",
      "pt_db.v3hp.PFFZ - Grade: Train Loss = 0.570911, Test Loss = 0.659414\n",
      "Completed evaluation for pt_db.v3hp.PFFZ\n",
      "\n",
      "Successfully evaluated 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate grade losses for all loaded models using fixed functions\n",
    "loss_results = []\n",
    "\n",
    "print(\"Calculating grade losses for all models...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_tag in tqdm(model_tags):\n",
    "    # Load model using the existing function\n",
    "    model, params = load_model_from_structure_with_txt(model_tag)\n",
    "    \n",
    "    if model is not None:\n",
    "        # Calculate grade losses using fixed function\n",
    "        loss_result = calculate_grade_model_losses_fixed(model, grade_dataset, model_tag)\n",
    "        loss_results.append(loss_result)\n",
    "        print(f\"Completed evaluation for {model_tag}\")\n",
    "    else:\n",
    "        print(f\"Failed to load model: {model_tag}\")\n",
    "\n",
    "print(f\"\\nSuccessfully evaluated {len(loss_results)} models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grade Task Loss Results Summary:\n",
      "================================================================================\n",
      "\n",
      "GRADE Task Losses:\n",
      "----------------------------------------\n",
      "         model_tag  grade_train_loss  grade_test_loss  overfitting\n",
      "0  pt_db.v3hp.RE5C          0.556246         0.659720     0.103474\n",
      "1  pt_db.v3hp.2HNP          0.571103         0.658976     0.087873\n",
      "2  pt_db.v3hp.CPUT          0.573922         0.651741     0.077819\n",
      "3  pt_db.v3hp.VPLV          0.558082         0.655647     0.097565\n",
      "4  pt_db.v3hp.PFFZ          0.570911         0.659414     0.088503\n",
      "\n",
      "Summary for Grade Task:\n",
      "  Average Train Loss: 0.566053\n",
      "  Average Test Loss: 0.657100\n",
      "  Average Overfitting: 0.091047\n",
      "  Best Test Loss: 0.651741 (pt_db.v3hp.CPUT)\n",
      "  Worst Test Loss: 0.659720 (pt_db.v3hp.RE5C)\n",
      "\n",
      "Grade loss results saved to evaluation_results/grade_model_loss_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Organize and display grade loss results\n",
    "loss_df = pd.DataFrame(loss_results)\n",
    "\n",
    "print(\"Grade Task Loss Results Summary:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display grade task results\n",
    "train_col = 'grade_train_loss'\n",
    "test_col = 'grade_test_loss'\n",
    "\n",
    "if train_col in loss_df.columns and test_col in loss_df.columns:\n",
    "    print(f\"\\nGRADE Task Losses:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    grade_results = loss_df[['model_tag', train_col, test_col]].copy()\n",
    "    grade_results['overfitting'] = grade_results[test_col] - grade_results[train_col]\n",
    "    \n",
    "    print(grade_results.round(6))\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nSummary for Grade Task:\")\n",
    "    print(f\"  Average Train Loss: {grade_results[train_col].mean():.6f}\")\n",
    "    print(f\"  Average Test Loss: {grade_results[test_col].mean():.6f}\")\n",
    "    print(f\"  Average Overfitting: {grade_results['overfitting'].mean():.6f}\")\n",
    "    print(f\"  Best Test Loss: {grade_results[test_col].min():.6f} ({grade_results.loc[grade_results[test_col].idxmin(), 'model_tag']})\")\n",
    "    print(f\"  Worst Test Loss: {grade_results[test_col].max():.6f} ({grade_results.loc[grade_results[test_col].idxmax(), 'model_tag']})\")\n",
    "\n",
    "# Save results\n",
    "loss_df.to_csv(RESULTS_DIR / 'grade_model_loss_results.csv', index=False)\n",
    "print(f\"\\nGrade loss results saved to {RESULTS_DIR / 'grade_model_loss_results.csv'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating classifier accuracy for individual models and ensemble...\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading models:  20%|██        | 1/5 [00:00<00:00,  9.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.RE5C\n",
      "pt_db.v3hp.RE5C: Test Acc = 0.769, Train Acc = 0.979\n",
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.2HNP\n",
      "pt_db.v3hp.2HNP: Test Acc = 0.770, Train Acc = 0.946\n",
      "{'model_params': {'encoder__hidden_layers': 50, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading models:  60%|██████    | 3/5 [00:00<00:00, 10.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model: pt_db.v3hp.CPUT\n",
      "pt_db.v3hp.CPUT: Test Acc = 0.793, Train Acc = 0.945\n",
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.VPLV\n",
      "pt_db.v3hp.VPLV: Test Acc = 0.787, Train Acc = 0.978\n",
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 400.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading models: 100%|██████████| 5/5 [00:00<00:00,  9.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model: pt_db.v3hp.PFFZ\n",
      "pt_db.v3hp.PFFZ: Test Acc = 0.783, Train Acc = 0.949\n",
      "\n",
      "ENSEMBLE: Test Accuracy = 0.815\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Classifier Accuracy for Ensemble\n",
    "def calculate_classifier_accuracy(y_true, y_pred, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate classification accuracy for binary classification\n",
    "    \n",
    "    Args:\n",
    "        y_true: True binary labels\n",
    "        y_pred: Predicted probabilities\n",
    "        threshold: Classification threshold (default 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing accuracy metrics\n",
    "    \"\"\"\n",
    "    # Convert predictions to binary using threshold\n",
    "    y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "    y_true_binary = y_true.astype(int)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(y_true_binary == y_pred_binary)\n",
    "    \n",
    "    # Calculate per-class accuracy\n",
    "    class_accuracies = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        class_acc = np.mean(y_true_binary[:, i] == y_pred_binary[:, i])\n",
    "        class_accuracies.append(class_acc)\n",
    "    \n",
    "    return {\n",
    "        'overall_accuracy': accuracy,\n",
    "        'class_accuracies': class_accuracies,\n",
    "        'y_pred_binary': y_pred_binary,\n",
    "        'y_true_binary': y_true_binary\n",
    "    }\n",
    "\n",
    "def calculate_ensemble_classifier_accuracy(model_tags, grade_dataset, threshold=0.5, device='cpu'):\n",
    "    \"\"\"\n",
    "    Calculate classifier accuracy for individual models and ensemble\n",
    "    \n",
    "    Args:\n",
    "        model_tags: List of model tags\n",
    "        grade_dataset: Dictionary containing train/test datasets\n",
    "        threshold: Classification threshold\n",
    "        device: Device to run evaluation on\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing accuracy results\n",
    "    \"\"\"\n",
    "    print(\"Calculating classifier accuracy for individual models and ensemble...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Store individual model predictions\n",
    "    individual_predictions = []\n",
    "    model_results = []\n",
    "    \n",
    "    # Get true values\n",
    "    y_test_true = grade_dataset['test_y'].numpy()\n",
    "    y_train_true = grade_dataset['train_y'].numpy()\n",
    "    grade_cols = TARGET_Y_COLS['grade']\n",
    "    \n",
    "    for model_tag in tqdm(model_tags, desc=\"Loading models\"):\n",
    "        # Load model\n",
    "        model, params = load_model_from_structure_with_txt(model_tag)\n",
    "        \n",
    "        if model is not None:\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            # Ensure model parameters are float32\n",
    "            for param in model.parameters():\n",
    "                param.data = param.data.float()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Test predictions\n",
    "                test_X = grade_dataset['test_X'].to(device).float()\n",
    "                test_pred = model.forward(test_X, 'grade').cpu().numpy()\n",
    "                \n",
    "                # Train predictions\n",
    "                train_X = grade_dataset['train_X'].to(device).float()\n",
    "                train_pred = model.forward(train_X, 'grade').cpu().numpy()\n",
    "                \n",
    "                # Store predictions for ensemble\n",
    "                individual_predictions.append(test_pred)\n",
    "                \n",
    "                # Calculate accuracy for this model\n",
    "                model_result = {'model_tag': model_tag}\n",
    "                \n",
    "                # Test accuracy\n",
    "                test_acc = calculate_classifier_accuracy(y_test_true, test_pred, threshold)\n",
    "                model_result['test_overall_accuracy'] = test_acc['overall_accuracy']\n",
    "                for i, col in enumerate(grade_cols):\n",
    "                    model_result[f'test_accuracy_{col}'] = test_acc['class_accuracies'][i]\n",
    "                \n",
    "                # Train accuracy\n",
    "                train_acc = calculate_classifier_accuracy(y_train_true, train_pred, threshold)\n",
    "                model_result['train_overall_accuracy'] = train_acc['overall_accuracy']\n",
    "                for i, col in enumerate(grade_cols):\n",
    "                    model_result[f'train_accuracy_{col}'] = train_acc['class_accuracies'][i]\n",
    "                \n",
    "                model_results.append(model_result)\n",
    "                \n",
    "                print(f\"{model_tag}: Test Acc = {test_acc['overall_accuracy']:.3f}, Train Acc = {train_acc['overall_accuracy']:.3f}\")\n",
    "    \n",
    "    # Calculate ensemble predictions (average of all models)\n",
    "    if individual_predictions:\n",
    "        ensemble_test_pred = np.mean(individual_predictions, axis=0)\n",
    "        \n",
    "        # Calculate ensemble accuracy\n",
    "        ensemble_result = {'model_tag': 'ensemble'}\n",
    "        ensemble_acc = calculate_classifier_accuracy(y_test_true, ensemble_test_pred, threshold)\n",
    "        \n",
    "        ensemble_result['test_overall_accuracy'] = ensemble_acc['overall_accuracy']\n",
    "        for i, col in enumerate(grade_cols):\n",
    "            ensemble_result[f'test_accuracy_{col}'] = ensemble_acc['class_accuracies'][i]\n",
    "        \n",
    "        model_results.append(ensemble_result)\n",
    "        \n",
    "        print(f\"\\nENSEMBLE: Test Accuracy = {ensemble_acc['overall_accuracy']:.3f}\")\n",
    "        print(\"=\" * 70)\n",
    "    \n",
    "    return model_results\n",
    "\n",
    "# Calculate classifier accuracy for all models and ensemble\n",
    "accuracy_results = calculate_ensemble_classifier_accuracy(model_tags, grade_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFIER ACCURACY ANALYSIS RESULTS\n",
      "================================================================================\n",
      "\n",
      "INDIVIDUAL MODEL ACCURACY RESULTS:\n",
      "------------------------------------------------------------\n",
      "         model_tag  test_overall_accuracy  train_overall_accuracy  overfitting\n",
      "2  pt_db.v3hp.CPUT                  0.793                   0.945        0.152\n",
      "3  pt_db.v3hp.VPLV                  0.787                   0.978        0.191\n",
      "4  pt_db.v3hp.PFFZ                  0.783                   0.949        0.166\n",
      "1  pt_db.v3hp.2HNP                  0.770                   0.946        0.176\n",
      "0  pt_db.v3hp.RE5C                  0.769                   0.979        0.211\n",
      "\n",
      "Best Individual Model: pt_db.v3hp.CPUT\n",
      "  Test Accuracy: 0.793\n",
      "  Train Accuracy: 0.945\n",
      "  Overfitting: 0.152\n",
      "\n",
      "Worst Individual Model: pt_db.v3hp.RE5C\n",
      "  Test Accuracy: 0.769\n",
      "  Train Accuracy: 0.979\n",
      "  Overfitting: 0.211\n",
      "\n",
      "ENSEMBLE RESULTS:\n",
      "----------------------------------------\n",
      "Ensemble Test Accuracy: 0.815\n",
      "\n",
      "COMPARISON:\n",
      "  Average Individual Accuracy: 0.780\n",
      "  Best Individual Accuracy: 0.793\n",
      "  Ensemble Accuracy: 0.815\n",
      "  Ensemble vs Average: +0.034\n",
      "  Ensemble vs Best: +0.022\n",
      "  ✅ Ensemble outperforms best individual model!\n",
      "\n",
      "PER-CLASS ACCURACY ANALYSIS:\n",
      "----------------------------------------\n",
      "Individual Models Average:\n",
      "  Detachability: 0.819\n",
      "  FlatnessUni: 0.706\n",
      "  Feasibility: 0.817\n",
      "\n",
      "Ensemble:\n",
      "  Detachability: 0.867\n",
      "  FlatnessUni: 0.733\n",
      "  Feasibility: 0.844\n",
      "\n",
      "Accuracy results saved to evaluation_results/classifier_accuracy_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Display and analyze classifier accuracy results\n",
    "accuracy_df = pd.DataFrame(accuracy_results)\n",
    "\n",
    "print(\"CLASSIFIER ACCURACY ANALYSIS RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display individual model accuracy results\n",
    "individual_models = accuracy_df[accuracy_df['model_tag'] != 'ensemble']\n",
    "ensemble_result = accuracy_df[accuracy_df['model_tag'] == 'ensemble']\n",
    "\n",
    "if not individual_models.empty:\n",
    "    print(\"\\nINDIVIDUAL MODEL ACCURACY RESULTS:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Create summary table\n",
    "    summary_cols = ['model_tag', 'test_overall_accuracy', 'train_overall_accuracy']\n",
    "    summary_df = individual_models[summary_cols].copy()\n",
    "    summary_df['overfitting'] = summary_df['train_overall_accuracy'] - summary_df['test_overall_accuracy']\n",
    "    \n",
    "    # Sort by test accuracy (descending)\n",
    "    summary_df = summary_df.sort_values('test_overall_accuracy', ascending=False)\n",
    "    \n",
    "    print(summary_df.round(3))\n",
    "    \n",
    "    # Best and worst individual models\n",
    "    best_model = summary_df.iloc[0]\n",
    "    worst_model = summary_df.iloc[-1]\n",
    "    \n",
    "    print(f\"\\nBest Individual Model: {best_model['model_tag']}\")\n",
    "    print(f\"  Test Accuracy: {best_model['test_overall_accuracy']:.3f}\")\n",
    "    print(f\"  Train Accuracy: {best_model['train_overall_accuracy']:.3f}\")\n",
    "    print(f\"  Overfitting: {best_model['overfitting']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nWorst Individual Model: {worst_model['model_tag']}\")\n",
    "    print(f\"  Test Accuracy: {worst_model['test_overall_accuracy']:.3f}\")\n",
    "    print(f\"  Train Accuracy: {worst_model['train_overall_accuracy']:.3f}\")\n",
    "    print(f\"  Overfitting: {worst_model['overfitting']:.3f}\")\n",
    "\n",
    "# Display ensemble results\n",
    "if not ensemble_result.empty:\n",
    "    ensemble_acc = ensemble_result.iloc[0]['test_overall_accuracy']\n",
    "    print(f\"\\nENSEMBLE RESULTS:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Ensemble Test Accuracy: {ensemble_acc:.3f}\")\n",
    "    \n",
    "    # Compare ensemble with individual models\n",
    "    if not individual_models.empty:\n",
    "        avg_individual_acc = individual_models['test_overall_accuracy'].mean()\n",
    "        best_individual_acc = individual_models['test_overall_accuracy'].max()\n",
    "        \n",
    "        print(f\"\\nCOMPARISON:\")\n",
    "        print(f\"  Average Individual Accuracy: {avg_individual_acc:.3f}\")\n",
    "        print(f\"  Best Individual Accuracy: {best_individual_acc:.3f}\")\n",
    "        print(f\"  Ensemble Accuracy: {ensemble_acc:.3f}\")\n",
    "        print(f\"  Ensemble vs Average: {ensemble_acc - avg_individual_acc:+.3f}\")\n",
    "        print(f\"  Ensemble vs Best: {ensemble_acc - best_individual_acc:+.3f}\")\n",
    "        \n",
    "        if ensemble_acc > best_individual_acc:\n",
    "            print(\"  ✅ Ensemble outperforms best individual model!\")\n",
    "        else:\n",
    "            print(\"  ❌ Ensemble does not outperform best individual model\")\n",
    "\n",
    "# Per-class accuracy analysis\n",
    "print(f\"\\nPER-CLASS ACCURACY ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "grade_cols = TARGET_Y_COLS['grade']\n",
    "\n",
    "if not individual_models.empty:\n",
    "    print(\"Individual Models Average:\")\n",
    "    for i, col in enumerate(grade_cols):\n",
    "        col_acc = individual_models[f'test_accuracy_{col}'].mean()\n",
    "        print(f\"  {col}: {col_acc:.3f}\")\n",
    "\n",
    "if not ensemble_result.empty:\n",
    "    print(\"\\nEnsemble:\")\n",
    "    for i, col in enumerate(grade_cols):\n",
    "        col_acc = ensemble_result.iloc[0][f'test_accuracy_{col}']\n",
    "        print(f\"  {col}: {col_acc:.3f}\")\n",
    "\n",
    "# Save accuracy results\n",
    "accuracy_df.to_csv(RESULTS_DIR / 'classifier_accuracy_results.csv', index=False)\n",
    "print(f\"\\nAccuracy results saved to {RESULTS_DIR / 'classifier_accuracy_results.csv'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-Fold Cross-Validation Ensemble Evaluation\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn as nn\n",
    "\n",
    "def create_kfold_splits(data_df, n_splits=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Create K-fold splits for cross-validation\n",
    "    \n",
    "    Args:\n",
    "        data_df: DataFrame containing the data\n",
    "        n_splits: Number of folds\n",
    "        random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        List of (train_idx, val_idx) tuples\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    splits = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(data_df):\n",
    "        splits.append((train_idx, val_idx))\n",
    "    \n",
    "    return splits\n",
    "\n",
    "def evaluate_ensemble_on_fold(model_tags, train_df, val_df, test_df, device='cpu'):\n",
    "    \"\"\"\n",
    "    Evaluate ensemble on a single fold\n",
    "    \n",
    "    Args:\n",
    "        model_tags: List of model tags\n",
    "        train_df: Training data for this fold\n",
    "        val_df: Validation data for this fold\n",
    "        test_df: Test data (same for all folds)\n",
    "        device: Device to run evaluation on\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing fold results\n",
    "    \"\"\"\n",
    "    # Prepare datasets for this fold\n",
    "    def prepare_fold_datasets(train_df, val_df, test_df):\n",
    "        # Extract input features and targets\n",
    "        X_train = train_df[X_COLS].values.astype(np.float32)\n",
    "        X_val = val_df[X_COLS].values.astype(np.float32)\n",
    "        X_test = test_df[X_COLS].values.astype(np.float32)\n",
    "        \n",
    "        grade_cols = TARGET_Y_COLS['grade']\n",
    "        y_train = train_df[grade_cols].values.astype(np.float32)\n",
    "        y_val = val_df[grade_cols].values.astype(np.float32)\n",
    "        y_test = test_df[grade_cols].values.astype(np.float32)\n",
    "        \n",
    "        return {\n",
    "            'train_X': torch.tensor(X_train, dtype=torch.float32),\n",
    "            'train_y': torch.tensor(y_train, dtype=torch.float32),\n",
    "            'val_X': torch.tensor(X_val, dtype=torch.float32),\n",
    "            'val_y': torch.tensor(y_val, dtype=torch.float32),\n",
    "            'test_X': torch.tensor(X_test, dtype=torch.float32),\n",
    "            'test_y': torch.tensor(y_test, dtype=torch.float32)\n",
    "        }\n",
    "    \n",
    "    fold_dataset = prepare_fold_datasets(train_df, val_df, test_df)\n",
    "    \n",
    "    # Store predictions from all models\n",
    "    train_predictions = []\n",
    "    val_predictions = []\n",
    "    test_predictions = []\n",
    "    \n",
    "    # Load and evaluate each model\n",
    "    for model_tag in model_tags:\n",
    "        model, params = load_model_from_structure_with_txt(model_tag)\n",
    "        \n",
    "        if model is not None:\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            # Ensure model parameters are float32\n",
    "            for param in model.parameters():\n",
    "                param.data = param.data.float()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Get predictions for all sets\n",
    "                train_pred = model.forward(fold_dataset['train_X'].to(device).float(), 'grade').cpu().numpy()\n",
    "                val_pred = model.forward(fold_dataset['val_X'].to(device).float(), 'grade').cpu().numpy()\n",
    "                test_pred = model.forward(fold_dataset['test_X'].to(device).float(), 'grade').cpu().numpy()\n",
    "                \n",
    "                train_predictions.append(train_pred)\n",
    "                val_predictions.append(val_pred)\n",
    "                test_predictions.append(test_pred)\n",
    "    \n",
    "    # Calculate ensemble predictions (average of all models)\n",
    "    if train_predictions:\n",
    "        ensemble_train_pred = np.mean(train_predictions, axis=0)\n",
    "        ensemble_val_pred = np.mean(val_predictions, axis=0)\n",
    "        ensemble_test_pred = np.mean(test_predictions, axis=0)\n",
    "        \n",
    "        # Calculate losses\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        train_loss = criterion(torch.tensor(ensemble_train_pred), fold_dataset['train_y']).item()\n",
    "        val_loss = criterion(torch.tensor(ensemble_val_pred), fold_dataset['val_y']).item()\n",
    "        test_loss = criterion(torch.tensor(ensemble_test_pred), fold_dataset['test_y']).item()\n",
    "        \n",
    "        # Calculate accuracies\n",
    "        train_acc = calculate_classifier_accuracy(fold_dataset['train_y'].numpy(), ensemble_train_pred)\n",
    "        val_acc = calculate_classifier_accuracy(fold_dataset['val_y'].numpy(), ensemble_val_pred)\n",
    "        test_acc = calculate_classifier_accuracy(fold_dataset['test_y'].numpy(), ensemble_test_pred)\n",
    "        \n",
    "        return {\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'test_loss': test_loss,\n",
    "            'train_accuracy': train_acc['overall_accuracy'],\n",
    "            'val_accuracy': val_acc['overall_accuracy'],\n",
    "            'test_accuracy': test_acc['overall_accuracy'],\n",
    "            'train_class_accuracies': train_acc['class_accuracies'],\n",
    "            'val_class_accuracies': val_acc['class_accuracies'],\n",
    "            'test_class_accuracies': test_acc['class_accuracies']\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "def run_5fold_ensemble_evaluation(model_tags, data_df, test_df, n_splits=5, random_state=42, device='cpu'):\n",
    "    \"\"\"\n",
    "    Run 5-fold cross-validation ensemble evaluation\n",
    "    \n",
    "    Args:\n",
    "        model_tags: List of model tags\n",
    "        data_df: Training data for cross-validation\n",
    "        test_df: Test data (same for all folds)\n",
    "        n_splits: Number of folds\n",
    "        random_state: Random seed\n",
    "        device: Device to run evaluation on\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing comprehensive results\n",
    "    \"\"\"\n",
    "    print(\"Running 5-Fold Cross-Validation Ensemble Evaluation\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create K-fold splits\n",
    "    splits = create_kfold_splits(data_df, n_splits, random_state)\n",
    "    fold_results = []\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(tqdm(splits, desc=\"Processing folds\")):\n",
    "        print(f\"\\nFold {fold_idx + 1}/{n_splits}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Split data\n",
    "        train_df = data_df.iloc[train_idx].copy()\n",
    "        val_df = data_df.iloc[val_idx].copy()\n",
    "        \n",
    "        print(f\"Train: {len(train_df)} samples, Val: {len(val_df)} samples, Test: {len(test_df)} samples\")\n",
    "        \n",
    "        # Evaluate ensemble on this fold\n",
    "        fold_result = evaluate_ensemble_on_fold(model_tags, train_df, val_df, test_df, device)\n",
    "        \n",
    "        if fold_result is not None:\n",
    "            fold_result['fold'] = fold_idx + 1\n",
    "            fold_results.append(fold_result)\n",
    "            \n",
    "            print(f\"  Train Loss: {fold_result['train_loss']:.6f}, Accuracy: {fold_result['train_accuracy']:.3f}\")\n",
    "            print(f\"  Val Loss: {fold_result['val_loss']:.6f}, Accuracy: {fold_result['val_accuracy']:.3f}\")\n",
    "            print(f\"  Test Loss: {fold_result['test_loss']:.6f}, Accuracy: {fold_result['test_accuracy']:.3f}\")\n",
    "        else:\n",
    "            print(f\"  ❌ Fold {fold_idx + 1} failed\")\n",
    "    \n",
    "    if not fold_results:\n",
    "        print(\"❌ All folds failed!\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate average results across folds\n",
    "    avg_results = {\n",
    "        'train_loss_mean': np.mean([r['train_loss'] for r in fold_results]),\n",
    "        'train_loss_std': np.std([r['train_loss'] for r in fold_results]),\n",
    "        'val_loss_mean': np.mean([r['val_loss'] for r in fold_results]),\n",
    "        'val_loss_std': np.std([r['val_loss'] for r in fold_results]),\n",
    "        'test_loss_mean': np.mean([r['test_loss'] for r in fold_results]),\n",
    "        'test_loss_std': np.std([r['test_loss'] for r in fold_results]),\n",
    "        'train_accuracy_mean': np.mean([r['train_accuracy'] for r in fold_results]),\n",
    "        'train_accuracy_std': np.std([r['train_accuracy'] for r in fold_results]),\n",
    "        'val_accuracy_mean': np.mean([r['val_accuracy'] for r in fold_results]),\n",
    "        'val_accuracy_std': np.std([r['val_accuracy'] for r in fold_results]),\n",
    "        'test_accuracy_mean': np.mean([r['test_accuracy'] for r in fold_results]),\n",
    "        'test_accuracy_std': np.std([r['test_accuracy'] for r in fold_results])\n",
    "    }\n",
    "    \n",
    "    # Calculate per-class accuracy averages\n",
    "    grade_cols = TARGET_Y_COLS['grade']\n",
    "    for i, col in enumerate(grade_cols):\n",
    "        avg_results[f'train_accuracy_{col}_mean'] = np.mean([r['train_class_accuracies'][i] for r in fold_results])\n",
    "        avg_results[f'train_accuracy_{col}_std'] = np.std([r['train_class_accuracies'][i] for r in fold_results])\n",
    "        avg_results[f'val_accuracy_{col}_mean'] = np.mean([r['val_class_accuracies'][i] for r in fold_results])\n",
    "        avg_results[f'val_accuracy_{col}_std'] = np.std([r['val_class_accuracies'][i] for r in fold_results])\n",
    "        avg_results[f'test_accuracy_{col}_mean'] = np.mean([r['test_class_accuracies'][i] for r in fold_results])\n",
    "        avg_results[f'test_accuracy_{col}_std'] = np.std([r['test_class_accuracies'][i] for r in fold_results])\n",
    "    \n",
    "    print(f\"\\n5-FOLD CROSS-VALIDATION RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Train Loss: {avg_results['train_loss_mean']:.6f} ± {avg_results['train_loss_std']:.6f}\")\n",
    "    print(f\"Val Loss: {avg_results['val_loss_mean']:.6f} ± {avg_results['val_loss_std']:.6f}\")\n",
    "    print(f\"Test Loss: {avg_results['test_loss_mean']:.6f} ± {avg_results['test_loss_std']:.6f}\")\n",
    "    print(f\"Train Accuracy: {avg_results['train_accuracy_mean']:.3f} ± {avg_results['train_accuracy_std']:.3f}\")\n",
    "    print(f\"Val Accuracy: {avg_results['val_accuracy_mean']:.3f} ± {avg_results['val_accuracy_std']:.3f}\")\n",
    "    print(f\"Test Accuracy: {avg_results['test_accuracy_mean']:.3f} ± {avg_results['test_accuracy_std']:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'fold_results': fold_results,\n",
    "        'average_results': avg_results,\n",
    "        'n_folds': len(fold_results)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-Fold Cross-Validation Ensemble Evaluation...\n",
      "Using 5 models: ['RE5C', '2HNP', 'CPUT', 'VPLV', 'PFFZ']\n",
      "Running 5-Fold Cross-Validation Ensemble Evaluation\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folds:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "------------------------------\n",
      "Train: 2118 samples, Val: 530 samples, Test: 180 samples\n",
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.RE5C\n",
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.2HNP\n",
      "{'model_params': {'encoder__hidden_layers': 50, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.CPUT\n",
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.VPLV\n",
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 400.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folds:  20%|██        | 1/5 [00:00<00:01,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model: pt_db.v3hp.PFFZ\n",
      "  Train Loss: 0.564545, Accuracy: 0.958\n",
      "  Val Loss: 0.562088, Accuracy: 0.953\n",
      "  Test Loss: 0.649876, Accuracy: 0.815\n",
      "\n",
      "Fold 2/5\n",
      "------------------------------\n",
      "Train: 2118 samples, Val: 530 samples, Test: 180 samples\n",
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.RE5C\n",
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.2HNP\n",
      "{'model_params': {'encoder__hidden_layers': 50, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.CPUT\n",
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.VPLV\n",
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 400.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folds:  40%|████      | 2/5 [00:00<00:01,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model: pt_db.v3hp.PFFZ\n",
      "  Train Loss: 0.564654, Accuracy: 0.959\n",
      "  Val Loss: 0.561653, Accuracy: 0.948\n",
      "  Test Loss: 0.649876, Accuracy: 0.815\n",
      "\n",
      "Fold 3/5\n",
      "------------------------------\n",
      "Train: 2118 samples, Val: 530 samples, Test: 180 samples\n",
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.RE5C\n",
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.2HNP\n",
      "{'model_params': {'encoder__hidden_layers': 50, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.CPUT\n",
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.VPLV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folds:  60%|██████    | 3/5 [00:01<00:00,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 400.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.PFFZ\n",
      "  Train Loss: 0.563259, Accuracy: 0.954\n",
      "  Val Loss: 0.567229, Accuracy: 0.970\n",
      "  Test Loss: 0.649876, Accuracy: 0.815\n",
      "\n",
      "Fold 4/5\n",
      "------------------------------\n",
      "Train: 2119 samples, Val: 529 samples, Test: 180 samples\n",
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.RE5C\n",
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.2HNP\n",
      "{'model_params': {'encoder__hidden_layers': 50, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.CPUT\n",
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.VPLV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folds:  80%|████████  | 4/5 [00:01<00:00,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 400.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.PFFZ\n",
      "  Train Loss: 0.562384, Accuracy: 0.957\n",
      "  Val Loss: 0.570737, Accuracy: 0.957\n",
      "  Test Loss: 0.649876, Accuracy: 0.815\n",
      "\n",
      "Fold 5/5\n",
      "------------------------------\n",
      "Train: 2119 samples, Val: 529 samples, Test: 180 samples\n",
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.RE5C\n",
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.2HNP\n",
      "{'model_params': {'encoder__hidden_layers': 50, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.CPUT\n",
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.VPLV\n",
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 400.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folds: 100%|██████████| 5/5 [00:02<00:00,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model: pt_db.v3hp.PFFZ\n",
      "  Train Loss: 0.565424, Accuracy: 0.957\n",
      "  Val Loss: 0.558561, Accuracy: 0.956\n",
      "  Test Loss: 0.649876, Accuracy: 0.815\n",
      "\n",
      "5-FOLD CROSS-VALIDATION RESULTS\n",
      "==================================================\n",
      "Train Loss: 0.564053 ± 0.001086\n",
      "Val Loss: 0.564054 ± 0.004348\n",
      "Test Loss: 0.649876 ± 0.000000\n",
      "Train Accuracy: 0.957 ± 0.002\n",
      "Val Accuracy: 0.957 ± 0.007\n",
      "Test Accuracy: 0.815 ± 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run 5-fold cross-validation ensemble evaluation\n",
    "print(\"Starting 5-Fold Cross-Validation Ensemble Evaluation...\")\n",
    "print(f\"Using {len(model_tags)} models: {[tag.split('.')[-1] for tag in model_tags]}\")\n",
    "\n",
    "# Run the evaluation\n",
    "cv_results = run_5fold_ensemble_evaluation(\n",
    "    model_tags=model_tags,\n",
    "    data_df=train_df,  # Use train_df for cross-validation\n",
    "    test_df=test_df,   # Use test_df as held-out test set\n",
    "    n_splits=5,\n",
    "    random_state=42,\n",
    "    device='cpu'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE 5-FOLD CROSS-VALIDATION ENSEMBLE RESULTS\n",
      "================================================================================\n",
      "\n",
      "DETAILED RESULTS SUMMARY:\n",
      "------------------------------------------------------------\n",
      "LOSS RESULTS:\n",
      "  Train Loss: 0.564053 ± 0.001086\n",
      "  Val Loss:   0.564054 ± 0.004348\n",
      "  Test Loss:  0.649876 ± 0.000000\n",
      "\n",
      "ACCURACY RESULTS:\n",
      "  Train Accuracy: 0.957 ± 0.002\n",
      "  Val Accuracy:   0.957 ± 0.007\n",
      "  Test Accuracy:  0.815 ± 0.000\n",
      "\n",
      "PER-CLASS ACCURACY RESULTS:\n",
      "  Detachability:\n",
      "    Train: 0.996 ± 0.001\n",
      "    Val:   0.996 ± 0.003\n",
      "    Test:  0.867 ± 0.000\n",
      "  FlatnessUni:\n",
      "    Train: 0.893 ± 0.005\n",
      "    Val:   0.893 ± 0.020\n",
      "    Test:  0.733 ± 0.000\n",
      "  Feasibility:\n",
      "    Train: 0.981 ± 0.001\n",
      "    Val:   0.981 ± 0.004\n",
      "    Test:  0.844 ± 0.000\n",
      "\n",
      "FOLD-BY-FOLD BREAKDOWN:\n",
      "----------------------------------------\n",
      "Fold 1:\n",
      "  Train: Loss=0.564545, Acc=0.958\n",
      "  Val:   Loss=0.562088, Acc=0.953\n",
      "  Test:  Loss=0.649876, Acc=0.815\n",
      "Fold 2:\n",
      "  Train: Loss=0.564654, Acc=0.959\n",
      "  Val:   Loss=0.561653, Acc=0.948\n",
      "  Test:  Loss=0.649876, Acc=0.815\n",
      "Fold 3:\n",
      "  Train: Loss=0.563259, Acc=0.954\n",
      "  Val:   Loss=0.567229, Acc=0.970\n",
      "  Test:  Loss=0.649876, Acc=0.815\n",
      "Fold 4:\n",
      "  Train: Loss=0.562384, Acc=0.957\n",
      "  Val:   Loss=0.570737, Acc=0.957\n",
      "  Test:  Loss=0.649876, Acc=0.815\n",
      "Fold 5:\n",
      "  Train: Loss=0.565424, Acc=0.957\n",
      "  Val:   Loss=0.558561, Acc=0.956\n",
      "  Test:  Loss=0.649876, Acc=0.815\n",
      "\n",
      "PERFORMANCE ANALYSIS:\n",
      "------------------------------\n",
      "Overfitting Analysis:\n",
      "  Loss Overfitting: 0.000000 (Val - Train)\n",
      "  Accuracy Overfitting: 0.000 (Train - Val)\n",
      "\n",
      "Generalization Analysis:\n",
      "  Loss Generalization: 0.085822 (Test - Val)\n",
      "  Accuracy Generalization: 0.142 (Val - Test)\n",
      "\n",
      "Results saved to:\n",
      "  - evaluation_results/5fold_cv_fold_results.csv\n",
      "  - evaluation_results/5fold_cv_summary_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive 5-Fold Cross-Validation Results Analysis\n",
    "if cv_results is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE 5-FOLD CROSS-VALIDATION ENSEMBLE RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    avg_results = cv_results['average_results']\n",
    "    fold_results = cv_results['fold_results']\n",
    "    \n",
    "    # Create detailed results table\n",
    "    print(\"\\nDETAILED RESULTS SUMMARY:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Loss results\n",
    "    print(\"LOSS RESULTS:\")\n",
    "    print(f\"  Train Loss: {avg_results['train_loss_mean']:.6f} ± {avg_results['train_loss_std']:.6f}\")\n",
    "    print(f\"  Val Loss:   {avg_results['val_loss_mean']:.6f} ± {avg_results['val_loss_std']:.6f}\")\n",
    "    print(f\"  Test Loss:  {avg_results['test_loss_mean']:.6f} ± {avg_results['test_loss_std']:.6f}\")\n",
    "    \n",
    "    # Accuracy results\n",
    "    print(\"\\nACCURACY RESULTS:\")\n",
    "    print(f\"  Train Accuracy: {avg_results['train_accuracy_mean']:.3f} ± {avg_results['train_accuracy_std']:.3f}\")\n",
    "    print(f\"  Val Accuracy:   {avg_results['val_accuracy_mean']:.3f} ± {avg_results['val_accuracy_std']:.3f}\")\n",
    "    print(f\"  Test Accuracy:  {avg_results['test_accuracy_mean']:.3f} ± {avg_results['test_accuracy_std']:.3f}\")\n",
    "    \n",
    "    # Per-class accuracy results\n",
    "    print(\"\\nPER-CLASS ACCURACY RESULTS:\")\n",
    "    grade_cols = TARGET_Y_COLS['grade']\n",
    "    for col in grade_cols:\n",
    "        train_mean = avg_results[f'train_accuracy_{col}_mean']\n",
    "        train_std = avg_results[f'train_accuracy_{col}_std']\n",
    "        val_mean = avg_results[f'val_accuracy_{col}_mean']\n",
    "        val_std = avg_results[f'val_accuracy_{col}_std']\n",
    "        test_mean = avg_results[f'test_accuracy_{col}_mean']\n",
    "        test_std = avg_results[f'test_accuracy_{col}_std']\n",
    "        \n",
    "        print(f\"  {col}:\")\n",
    "        print(f\"    Train: {train_mean:.3f} ± {train_std:.3f}\")\n",
    "        print(f\"    Val:   {val_mean:.3f} ± {val_std:.3f}\")\n",
    "        print(f\"    Test:  {test_mean:.3f} ± {test_std:.3f}\")\n",
    "    \n",
    "    # Fold-by-fold breakdown\n",
    "    print(f\"\\nFOLD-BY-FOLD BREAKDOWN:\")\n",
    "    print(\"-\" * 40)\n",
    "    fold_df = pd.DataFrame(fold_results)\n",
    "    \n",
    "    for i, row in fold_df.iterrows():\n",
    "        print(f\"Fold {int(row['fold'])}:\")\n",
    "        print(f\"  Train: Loss={row['train_loss']:.6f}, Acc={row['train_accuracy']:.3f}\")\n",
    "        print(f\"  Val:   Loss={row['val_loss']:.6f}, Acc={row['val_accuracy']:.3f}\")\n",
    "        print(f\"  Test:  Loss={row['test_loss']:.6f}, Acc={row['test_accuracy']:.3f}\")\n",
    "    \n",
    "    # Performance analysis\n",
    "    print(f\"\\nPERFORMANCE ANALYSIS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Overfitting analysis\n",
    "    train_val_loss_diff = avg_results['val_loss_mean'] - avg_results['train_loss_mean']\n",
    "    train_val_acc_diff = avg_results['train_accuracy_mean'] - avg_results['val_accuracy_mean']\n",
    "    \n",
    "    print(f\"Overfitting Analysis:\")\n",
    "    print(f\"  Loss Overfitting: {train_val_loss_diff:.6f} (Val - Train)\")\n",
    "    print(f\"  Accuracy Overfitting: {train_val_acc_diff:.3f} (Train - Val)\")\n",
    "    \n",
    "    # Generalization analysis\n",
    "    val_test_loss_diff = avg_results['test_loss_mean'] - avg_results['val_loss_mean']\n",
    "    val_test_acc_diff = avg_results['val_accuracy_mean'] - avg_results['test_accuracy_mean']\n",
    "    \n",
    "    print(f\"\\nGeneralization Analysis:\")\n",
    "    print(f\"  Loss Generalization: {val_test_loss_diff:.6f} (Test - Val)\")\n",
    "    print(f\"  Accuracy Generalization: {val_test_acc_diff:.3f} (Val - Test)\")\n",
    "    \n",
    "    # Save comprehensive results\n",
    "    results_summary = {\n",
    "        'evaluation_type': '5_fold_cross_validation_ensemble',\n",
    "        'n_models': len(model_tags),\n",
    "        'model_tags': model_tags,\n",
    "        'n_folds': cv_results['n_folds'],\n",
    "        'average_results': avg_results,\n",
    "        'fold_results': fold_results\n",
    "    }\n",
    "    \n",
    "    # Save to files\n",
    "    pd.DataFrame(fold_results).to_csv(RESULTS_DIR / '5fold_cv_fold_results.csv', index=False)\n",
    "    \n",
    "    # Save summary results\n",
    "    summary_df = pd.DataFrame([avg_results])\n",
    "    summary_df.to_csv(RESULTS_DIR / '5fold_cv_summary_results.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nResults saved to:\")\n",
    "    print(f\"  - {RESULTS_DIR / '5fold_cv_fold_results.csv'}\")\n",
    "    print(f\"  - {RESULTS_DIR / '5fold_cv_summary_results.csv'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Cross-validation evaluation failed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-Fold Cross-Validation Individual Model Evaluation...\n",
      "Evaluating 5 models: ['RE5C', '2HNP', 'CPUT', 'VPLV', 'PFFZ']\n",
      "Running 5-Fold Cross-Validation Individual Model Evaluation\n",
      "======================================================================\n",
      "\n",
      "Evaluating Model: pt_db.v3hp.RE5C\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing RE5C:  20%|██        | 1/5 [00:00<00:00,  8.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.RE5C\n",
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.RE5C\n",
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing RE5C: 100%|██████████| 5/5 [00:00<00:00, 12.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model: pt_db.v3hp.RE5C\n",
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.RE5C\n",
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.RE5C\n",
      "  Train Loss: 0.556246 ± 0.001224\n",
      "  Val Loss:   0.556248 ± 0.004898\n",
      "  Test Loss:  0.659720 ± 0.000000\n",
      "  Train Acc:  0.979 ± 0.001\n",
      "  Val Acc:    0.979 ± 0.004\n",
      "  Test Acc:   0.769 ± 0.000\n",
      "\n",
      "Evaluating Model: pt_db.v3hp.2HNP\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2HNP:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2HNP:  40%|████      | 2/5 [00:00<00:00, 12.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model: pt_db.v3hp.2HNP\n",
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.2HNP\n",
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.2HNP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2HNP: 100%|██████████| 5/5 [00:00<00:00, 13.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.2HNP\n",
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.2HNP\n",
      "  Train Loss: 0.571103 ± 0.000925\n",
      "  Val Loss:   0.571103 ± 0.003705\n",
      "  Test Loss:  0.658976 ± 0.000000\n",
      "  Train Acc:  0.946 ± 0.001\n",
      "  Val Acc:    0.946 ± 0.004\n",
      "  Test Acc:   0.770 ± 0.000\n",
      "\n",
      "Evaluating Model: pt_db.v3hp.CPUT\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CPUT:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'encoder__hidden_layers': 50, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.CPUT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CPUT:  20%|██        | 1/5 [00:00<00:00,  9.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'encoder__hidden_layers': 50, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.CPUT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CPUT:  40%|████      | 2/5 [00:00<00:00,  9.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'encoder__hidden_layers': 50, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.CPUT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CPUT:  60%|██████    | 3/5 [00:00<00:00,  9.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'encoder__hidden_layers': 50, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.CPUT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CPUT:  80%|████████  | 4/5 [00:00<00:00,  9.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'encoder__hidden_layers': 50, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.CPUT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CPUT: 100%|██████████| 5/5 [00:00<00:00,  9.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.573922 ± 0.001043\n",
      "  Val Loss:   0.573922 ± 0.004175\n",
      "  Test Loss:  0.651741 ± 0.000000\n",
      "  Train Acc:  0.945 ± 0.001\n",
      "  Val Acc:    0.945 ± 0.006\n",
      "  Test Acc:   0.793 ± 0.000\n",
      "\n",
      "Evaluating Model: pt_db.v3hp.VPLV\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing VPLV:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.VPLV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing VPLV:  40%|████      | 2/5 [00:00<00:00, 10.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.VPLV\n",
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.VPLV\n",
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing VPLV:  80%|████████  | 4/5 [00:00<00:00, 10.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model: pt_db.v3hp.VPLV\n",
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.VPLV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing VPLV: 100%|██████████| 5/5 [00:00<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.558083 ± 0.001188\n",
      "  Val Loss:   0.558083 ± 0.004753\n",
      "  Test Loss:  0.655647 ± 0.000000\n",
      "  Train Acc:  0.978 ± 0.001\n",
      "  Val Acc:    0.978 ± 0.004\n",
      "  Test Acc:   0.787 ± 0.000\n",
      "\n",
      "Evaluating Model: pt_db.v3hp.PFFZ\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PFFZ:  20%|██        | 1/5 [00:00<00:00,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 400.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.PFFZ\n",
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 400.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.PFFZ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PFFZ:  60%|██████    | 3/5 [00:00<00:00,  9.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 400.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.PFFZ\n",
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 400.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.PFFZ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PFFZ: 100%|██████████| 5/5 [00:00<00:00,  9.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 400.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.PFFZ\n",
      "  Train Loss: 0.570911 ± 0.001263\n",
      "  Val Loss:   0.570911 ± 0.005057\n",
      "  Test Loss:  0.659414 ± 0.000000\n",
      "  Train Acc:  0.949 ± 0.002\n",
      "  Val Acc:    0.949 ± 0.008\n",
      "  Test Acc:   0.783 ± 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Individual Model Performance Evaluation\n",
    "def evaluate_individual_model_on_fold(model_tag, train_df, val_df, test_df, device='cpu'):\n",
    "    \"\"\"\n",
    "    Evaluate a single model on a fold\n",
    "    \n",
    "    Args:\n",
    "        model_tag: Model tag to evaluate\n",
    "        train_df: Training data for this fold\n",
    "        val_df: Validation data for this fold\n",
    "        test_df: Test data (same for all folds)\n",
    "        device: Device to run evaluation on\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing individual model results\n",
    "    \"\"\"\n",
    "    # Prepare datasets for this fold\n",
    "    def prepare_fold_datasets(train_df, val_df, test_df):\n",
    "        # Extract input features and targets\n",
    "        X_train = train_df[X_COLS].values.astype(np.float32)\n",
    "        X_val = val_df[X_COLS].values.astype(np.float32)\n",
    "        X_test = test_df[X_COLS].values.astype(np.float32)\n",
    "        \n",
    "        grade_cols = TARGET_Y_COLS['grade']\n",
    "        y_train = train_df[grade_cols].values.astype(np.float32)\n",
    "        y_val = val_df[grade_cols].values.astype(np.float32)\n",
    "        y_test = test_df[grade_cols].values.astype(np.float32)\n",
    "        \n",
    "        return {\n",
    "            'train_X': torch.tensor(X_train, dtype=torch.float32),\n",
    "            'train_y': torch.tensor(y_train, dtype=torch.float32),\n",
    "            'val_X': torch.tensor(X_val, dtype=torch.float32),\n",
    "            'val_y': torch.tensor(y_val, dtype=torch.float32),\n",
    "            'test_X': torch.tensor(X_test, dtype=torch.float32),\n",
    "            'test_y': torch.tensor(y_test, dtype=torch.float32)\n",
    "        }\n",
    "    \n",
    "    fold_dataset = prepare_fold_datasets(train_df, val_df, test_df)\n",
    "    \n",
    "    # Load and evaluate the model\n",
    "    model, params = load_model_from_structure_with_txt(model_tag)\n",
    "    \n",
    "    if model is not None:\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        # Ensure model parameters are float32\n",
    "        for param in model.parameters():\n",
    "            param.data = param.data.float()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get predictions for all sets\n",
    "            train_pred = model.forward(fold_dataset['train_X'].to(device).float(), 'grade').cpu().numpy()\n",
    "            val_pred = model.forward(fold_dataset['val_X'].to(device).float(), 'grade').cpu().numpy()\n",
    "            test_pred = model.forward(fold_dataset['test_X'].to(device).float(), 'grade').cpu().numpy()\n",
    "            \n",
    "            # Calculate losses\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            \n",
    "            train_loss = criterion(torch.tensor(train_pred), fold_dataset['train_y']).item()\n",
    "            val_loss = criterion(torch.tensor(val_pred), fold_dataset['val_y']).item()\n",
    "            test_loss = criterion(torch.tensor(test_pred), fold_dataset['test_y']).item()\n",
    "            \n",
    "            # Calculate accuracies\n",
    "            train_acc = calculate_classifier_accuracy(fold_dataset['train_y'].numpy(), train_pred)\n",
    "            val_acc = calculate_classifier_accuracy(fold_dataset['val_y'].numpy(), val_pred)\n",
    "            test_acc = calculate_classifier_accuracy(fold_dataset['test_y'].numpy(), test_pred)\n",
    "            \n",
    "            return {\n",
    "                'model_tag': model_tag,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'test_loss': test_loss,\n",
    "                'train_accuracy': train_acc['overall_accuracy'],\n",
    "                'val_accuracy': val_acc['overall_accuracy'],\n",
    "                'test_accuracy': test_acc['overall_accuracy'],\n",
    "                'train_class_accuracies': train_acc['class_accuracies'],\n",
    "                'val_class_accuracies': val_acc['class_accuracies'],\n",
    "                'test_class_accuracies': test_acc['class_accuracies']\n",
    "            }\n",
    "    \n",
    "    return None\n",
    "\n",
    "def run_5fold_individual_model_evaluation(model_tags, data_df, test_df, n_splits=5, random_state=42, device='cpu'):\n",
    "    \"\"\"\n",
    "    Run 5-fold cross-validation for individual models\n",
    "    \n",
    "    Args:\n",
    "        model_tags: List of model tags\n",
    "        data_df: Training data for cross-validation\n",
    "        test_df: Test data (same for all folds)\n",
    "        n_splits: Number of folds\n",
    "        random_state: Random seed\n",
    "        device: Device to run evaluation on\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing individual model results\n",
    "    \"\"\"\n",
    "    print(\"Running 5-Fold Cross-Validation Individual Model Evaluation\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create K-fold splits\n",
    "    splits = create_kfold_splits(data_df, n_splits, random_state)\n",
    "    all_model_results = []\n",
    "    \n",
    "    for model_tag in model_tags:\n",
    "        print(f\"\\nEvaluating Model: {model_tag}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        model_fold_results = []\n",
    "        \n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(tqdm(splits, desc=f\"Processing {model_tag.split('.')[-1]}\")):\n",
    "            # Split data\n",
    "            train_df = data_df.iloc[train_idx].copy()\n",
    "            val_df = data_df.iloc[val_idx].copy()\n",
    "            \n",
    "            # Evaluate model on this fold\n",
    "            fold_result = evaluate_individual_model_on_fold(model_tag, train_df, val_df, test_df, device)\n",
    "            \n",
    "            if fold_result is not None:\n",
    "                fold_result['fold'] = fold_idx + 1\n",
    "                model_fold_results.append(fold_result)\n",
    "        \n",
    "        if model_fold_results:\n",
    "            # Calculate average results for this model\n",
    "            avg_results = {\n",
    "                'model_tag': model_tag,\n",
    "                'train_loss_mean': np.mean([r['train_loss'] for r in model_fold_results]),\n",
    "                'train_loss_std': np.std([r['train_loss'] for r in model_fold_results]),\n",
    "                'val_loss_mean': np.mean([r['val_loss'] for r in model_fold_results]),\n",
    "                'val_loss_std': np.std([r['val_loss'] for r in model_fold_results]),\n",
    "                'test_loss_mean': np.mean([r['test_loss'] for r in model_fold_results]),\n",
    "                'test_loss_std': np.std([r['test_loss'] for r in model_fold_results]),\n",
    "                'train_accuracy_mean': np.mean([r['train_accuracy'] for r in model_fold_results]),\n",
    "                'train_accuracy_std': np.std([r['train_accuracy'] for r in model_fold_results]),\n",
    "                'val_accuracy_mean': np.mean([r['val_accuracy'] for r in model_fold_results]),\n",
    "                'val_accuracy_std': np.std([r['val_accuracy'] for r in model_fold_results]),\n",
    "                'test_accuracy_mean': np.mean([r['test_accuracy'] for r in model_fold_results]),\n",
    "                'test_accuracy_std': np.std([r['test_accuracy'] for r in model_fold_results])\n",
    "            }\n",
    "            \n",
    "            # Calculate per-class accuracy averages\n",
    "            grade_cols = TARGET_Y_COLS['grade']\n",
    "            for i, col in enumerate(grade_cols):\n",
    "                avg_results[f'train_accuracy_{col}_mean'] = np.mean([r['train_class_accuracies'][i] for r in model_fold_results])\n",
    "                avg_results[f'train_accuracy_{col}_std'] = np.std([r['train_class_accuracies'][i] for r in model_fold_results])\n",
    "                avg_results[f'val_accuracy_{col}_mean'] = np.mean([r['val_class_accuracies'][i] for r in model_fold_results])\n",
    "                avg_results[f'val_accuracy_{col}_std'] = np.std([r['val_class_accuracies'][i] for r in model_fold_results])\n",
    "                avg_results[f'test_accuracy_{col}_mean'] = np.mean([r['test_class_accuracies'][i] for r in model_fold_results])\n",
    "                avg_results[f'test_accuracy_{col}_std'] = np.std([r['test_class_accuracies'][i] for r in model_fold_results])\n",
    "            \n",
    "            all_model_results.append({\n",
    "                'model_tag': model_tag,\n",
    "                'fold_results': model_fold_results,\n",
    "                'average_results': avg_results,\n",
    "                'n_folds': len(model_fold_results)\n",
    "            })\n",
    "            \n",
    "            print(f\"  Train Loss: {avg_results['train_loss_mean']:.6f} ± {avg_results['train_loss_std']:.6f}\")\n",
    "            print(f\"  Val Loss:   {avg_results['val_loss_mean']:.6f} ± {avg_results['val_loss_std']:.6f}\")\n",
    "            print(f\"  Test Loss:  {avg_results['test_loss_mean']:.6f} ± {avg_results['test_loss_std']:.6f}\")\n",
    "            print(f\"  Train Acc:  {avg_results['train_accuracy_mean']:.3f} ± {avg_results['train_accuracy_std']:.3f}\")\n",
    "            print(f\"  Val Acc:    {avg_results['val_accuracy_mean']:.3f} ± {avg_results['val_accuracy_std']:.3f}\")\n",
    "            print(f\"  Test Acc:   {avg_results['test_accuracy_mean']:.3f} ± {avg_results['test_accuracy_std']:.3f}\")\n",
    "        else:\n",
    "            print(f\"  ❌ Model {model_tag} failed on all folds\")\n",
    "    \n",
    "    return all_model_results\n",
    "\n",
    "# Run individual model evaluation\n",
    "print(\"Starting 5-Fold Cross-Validation Individual Model Evaluation...\")\n",
    "print(f\"Evaluating {len(model_tags)} models: {[tag.split('.')[-1] for tag in model_tags]}\")\n",
    "\n",
    "individual_model_results = run_5fold_individual_model_evaluation(\n",
    "    model_tags=model_tags,\n",
    "    data_df=train_df,\n",
    "    test_df=test_df,\n",
    "    n_splits=5,\n",
    "    random_state=42,\n",
    "    device='cpu'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INDIVIDUAL MODEL 5-FOLD CROSS-VALIDATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "COMPREHENSIVE MODEL COMPARISON:\n",
      "--------------------------------------------------------------------------------\n",
      "Model          Train_Loss            Val_Loss           Test_Loss     Train_Acc       Val_Acc      Test_Acc Overfitting_Loss Overfitting_Acc\n",
      " RE5C 0.556246 ± 0.001224 0.556248 ± 0.004898 0.659720 ± 0.000000 0.979 ± 0.001 0.979 ± 0.004 0.769 ± 0.000         0.000001           0.000\n",
      " 2HNP 0.571103 ± 0.000925 0.571103 ± 0.003705 0.658976 ± 0.000000 0.946 ± 0.001 0.946 ± 0.004 0.770 ± 0.000         0.000000          -0.000\n",
      " CPUT 0.573922 ± 0.001043 0.573922 ± 0.004175 0.651741 ± 0.000000 0.945 ± 0.001 0.945 ± 0.006 0.793 ± 0.000        -0.000000          -0.000\n",
      " VPLV 0.558083 ± 0.001188 0.558083 ± 0.004753 0.655647 ± 0.000000 0.978 ± 0.001 0.978 ± 0.004 0.787 ± 0.000         0.000001           0.000\n",
      " PFFZ 0.570911 ± 0.001263 0.570911 ± 0.005057 0.659414 ± 0.000000 0.949 ± 0.002 0.949 ± 0.008 0.783 ± 0.000        -0.000001          -0.000\n",
      "\n",
      "MODEL RANKING:\n",
      "----------------------------------------\n",
      "By Test Accuracy:\n",
      "  1. CPUT: 0.793\n",
      "  2. VPLV: 0.787\n",
      "  3. PFFZ: 0.783\n",
      "  4. 2HNP: 0.770\n",
      "  5. RE5C: 0.769\n",
      "\n",
      "By Test Loss (lower is better):\n",
      "  1. CPUT: 0.651741\n",
      "  2. VPLV: 0.655647\n",
      "  3. 2HNP: 0.658976\n",
      "  4. PFFZ: 0.659414\n",
      "  5. RE5C: 0.659720\n",
      "\n",
      "BEST MODEL: CPUT\n",
      "------------------------------\n",
      "  Test Accuracy: 0.793 ± 0.000\n",
      "  Test Loss: 0.651741 ± 0.000000\n",
      "  Val Accuracy: 0.945 ± 0.006\n",
      "  Overfitting: -0.000\n",
      "\n",
      "WORST MODEL: RE5C\n",
      "------------------------------\n",
      "  Test Accuracy: 0.769 ± 0.000\n",
      "  Test Loss: 0.659720 ± 0.000000\n",
      "  Val Accuracy: 0.979 ± 0.004\n",
      "  Overfitting: 0.000\n",
      "\n",
      "PER-CLASS ACCURACY ANALYSIS:\n",
      "----------------------------------------\n",
      "\n",
      "Detachability:\n",
      "  1. VPLV: 0.844\n",
      "  2. PFFZ: 0.817\n",
      "  3. RE5C: 0.811\n",
      "  4. 2HNP: 0.811\n",
      "  5. CPUT: 0.811\n",
      "\n",
      "FlatnessUni:\n",
      "  1. CPUT: 0.733\n",
      "  2. PFFZ: 0.717\n",
      "  3. 2HNP: 0.694\n",
      "  4. VPLV: 0.694\n",
      "  5. RE5C: 0.689\n",
      "\n",
      "Feasibility:\n",
      "  1. CPUT: 0.833\n",
      "  2. VPLV: 0.822\n",
      "  3. PFFZ: 0.817\n",
      "  4. RE5C: 0.806\n",
      "  5. 2HNP: 0.806\n",
      "\n",
      "ENSEMBLE vs INDIVIDUAL MODELS COMPARISON:\n",
      "--------------------------------------------------\n",
      "Ensemble Test Accuracy: 0.815 ± 0.000\n",
      "Best Individual Test Accuracy: 0.793 ± 0.000\n",
      "Worst Individual Test Accuracy: 0.769 ± 0.000\n",
      "\n",
      "Ensemble vs Best Individual: +0.022\n",
      "Ensemble vs Worst Individual: +0.046\n",
      "✅ Ensemble outperforms best individual model!\n",
      "\n",
      "Individual model results saved to:\n",
      "  - evaluation_results/individual_models_5fold_results.csv\n",
      "  - evaluation_results/individual_models_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Individual Model Results Analysis and Comparison\n",
    "if individual_model_results:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"INDIVIDUAL MODEL 5-FOLD CROSS-VALIDATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create summary table for all models\n",
    "    summary_data = []\n",
    "    for model_result in individual_model_results:\n",
    "        avg_results = model_result['average_results']\n",
    "        model_name = model_result['model_tag'].split('.')[-1]\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Model': model_name,\n",
    "            'Train_Loss': f\"{avg_results['train_loss_mean']:.6f} ± {avg_results['train_loss_std']:.6f}\",\n",
    "            'Val_Loss': f\"{avg_results['val_loss_mean']:.6f} ± {avg_results['val_loss_std']:.6f}\",\n",
    "            'Test_Loss': f\"{avg_results['test_loss_mean']:.6f} ± {avg_results['test_loss_std']:.6f}\",\n",
    "            'Train_Acc': f\"{avg_results['train_accuracy_mean']:.3f} ± {avg_results['train_accuracy_std']:.3f}\",\n",
    "            'Val_Acc': f\"{avg_results['val_accuracy_mean']:.3f} ± {avg_results['val_accuracy_std']:.3f}\",\n",
    "            'Test_Acc': f\"{avg_results['test_accuracy_mean']:.3f} ± {avg_results['test_accuracy_std']:.3f}\",\n",
    "            'Overfitting_Loss': f\"{avg_results['val_loss_mean'] - avg_results['train_loss_mean']:.6f}\",\n",
    "            'Overfitting_Acc': f\"{avg_results['train_accuracy_mean'] - avg_results['val_accuracy_mean']:.3f}\"\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(\"\\nCOMPREHENSIVE MODEL COMPARISON:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Find best and worst models\n",
    "    print(f\"\\nMODEL RANKING:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Sort by test accuracy (descending)\n",
    "    sorted_models = sorted(individual_model_results, \n",
    "                          key=lambda x: x['average_results']['test_accuracy_mean'], \n",
    "                          reverse=True)\n",
    "    \n",
    "    print(\"By Test Accuracy:\")\n",
    "    for i, model_result in enumerate(sorted_models):\n",
    "        model_name = model_result['model_tag'].split('.')[-1]\n",
    "        test_acc = model_result['average_results']['test_accuracy_mean']\n",
    "        print(f\"  {i+1}. {model_name}: {test_acc:.3f}\")\n",
    "    \n",
    "    # Sort by test loss (ascending)\n",
    "    sorted_models_loss = sorted(individual_model_results, \n",
    "                               key=lambda x: x['average_results']['test_loss_mean'])\n",
    "    \n",
    "    print(\"\\nBy Test Loss (lower is better):\")\n",
    "    for i, model_result in enumerate(sorted_models_loss):\n",
    "        model_name = model_result['model_tag'].split('.')[-1]\n",
    "        test_loss = model_result['average_results']['test_loss_mean']\n",
    "        print(f\"  {i+1}. {model_name}: {test_loss:.6f}\")\n",
    "    \n",
    "    # Best and worst model analysis\n",
    "    best_model = sorted_models[0]\n",
    "    worst_model = sorted_models[-1]\n",
    "    \n",
    "    print(f\"\\nBEST MODEL: {best_model['model_tag'].split('.')[-1]}\")\n",
    "    print(\"-\" * 30)\n",
    "    best_avg = best_model['average_results']\n",
    "    print(f\"  Test Accuracy: {best_avg['test_accuracy_mean']:.3f} ± {best_avg['test_accuracy_std']:.3f}\")\n",
    "    print(f\"  Test Loss: {best_avg['test_loss_mean']:.6f} ± {best_avg['test_loss_std']:.6f}\")\n",
    "    print(f\"  Val Accuracy: {best_avg['val_accuracy_mean']:.3f} ± {best_avg['val_accuracy_std']:.3f}\")\n",
    "    print(f\"  Overfitting: {best_avg['train_accuracy_mean'] - best_avg['val_accuracy_mean']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nWORST MODEL: {worst_model['model_tag'].split('.')[-1]}\")\n",
    "    print(\"-\" * 30)\n",
    "    worst_avg = worst_model['average_results']\n",
    "    print(f\"  Test Accuracy: {worst_avg['test_accuracy_mean']:.3f} ± {worst_avg['test_accuracy_std']:.3f}\")\n",
    "    print(f\"  Test Loss: {worst_avg['test_loss_mean']:.6f} ± {worst_avg['test_loss_std']:.6f}\")\n",
    "    print(f\"  Val Accuracy: {worst_avg['val_accuracy_mean']:.3f} ± {worst_avg['val_accuracy_std']:.3f}\")\n",
    "    print(f\"  Overfitting: {worst_avg['train_accuracy_mean'] - worst_avg['val_accuracy_mean']:.3f}\")\n",
    "    \n",
    "    # Per-class accuracy analysis\n",
    "    print(f\"\\nPER-CLASS ACCURACY ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    grade_cols = TARGET_Y_COLS['grade']\n",
    "    \n",
    "    for col in grade_cols:\n",
    "        print(f\"\\n{col}:\")\n",
    "        class_results = []\n",
    "        for model_result in individual_model_results:\n",
    "            model_name = model_result['model_tag'].split('.')[-1]\n",
    "            test_acc = model_result['average_results'][f'test_accuracy_{col}_mean']\n",
    "            class_results.append((model_name, test_acc))\n",
    "        \n",
    "        # Sort by accuracy\n",
    "        class_results.sort(key=lambda x: x[1], reverse=True)\n",
    "        for i, (model_name, acc) in enumerate(class_results):\n",
    "            print(f\"  {i+1}. {model_name}: {acc:.3f}\")\n",
    "    \n",
    "    # Compare with ensemble results\n",
    "    if cv_results is not None:\n",
    "        ensemble_avg = cv_results['average_results']\n",
    "        print(f\"\\nENSEMBLE vs INDIVIDUAL MODELS COMPARISON:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        print(f\"Ensemble Test Accuracy: {ensemble_avg['test_accuracy_mean']:.3f} ± {ensemble_avg['test_accuracy_std']:.3f}\")\n",
    "        print(f\"Best Individual Test Accuracy: {best_avg['test_accuracy_mean']:.3f} ± {best_avg['test_accuracy_std']:.3f}\")\n",
    "        print(f\"Worst Individual Test Accuracy: {worst_avg['test_accuracy_mean']:.3f} ± {worst_avg['test_accuracy_std']:.3f}\")\n",
    "        \n",
    "        ensemble_vs_best = ensemble_avg['test_accuracy_mean'] - best_avg['test_accuracy_mean']\n",
    "        ensemble_vs_worst = ensemble_avg['test_accuracy_mean'] - worst_avg['test_accuracy_mean']\n",
    "        \n",
    "        print(f\"\\nEnsemble vs Best Individual: {ensemble_vs_best:+.3f}\")\n",
    "        print(f\"Ensemble vs Worst Individual: {ensemble_vs_worst:+.3f}\")\n",
    "        \n",
    "        if ensemble_vs_best > 0:\n",
    "            print(\"✅ Ensemble outperforms best individual model!\")\n",
    "        else:\n",
    "            print(\"❌ Ensemble does not outperform best individual model\")\n",
    "    \n",
    "    # Save individual model results\n",
    "    all_fold_results = []\n",
    "    for model_result in individual_model_results:\n",
    "        for fold_result in model_result['fold_results']:\n",
    "            all_fold_results.append(fold_result)\n",
    "    \n",
    "    pd.DataFrame(all_fold_results).to_csv(RESULTS_DIR / 'individual_models_5fold_results.csv', index=False)\n",
    "    \n",
    "    # Save summary results\n",
    "    individual_summary = []\n",
    "    for model_result in individual_model_results:\n",
    "        individual_summary.append(model_result['average_results'])\n",
    "    \n",
    "    pd.DataFrame(individual_summary).to_csv(RESULTS_DIR / 'individual_models_summary.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nIndividual model results saved to:\")\n",
    "    print(f\"  - {RESULTS_DIR / 'individual_models_5fold_results.csv'}\")\n",
    "    print(f\"  - {RESULTS_DIR / 'individual_models_summary.csv'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No individual model results to analyze!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN DEMO DATASET (343 datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN Demo dataset shape: (342, 42)\n",
      "Train set: 273 samples\n",
      "Test set: 69 samples\n",
      "Target columns: ['TensileStress', 'TensileStrain', 'TensileModulusLog10', 'TensileToughnessMean100n90', 'TransVis', 'TransIR', 'TransUV']\n"
     ]
    }
   ],
   "source": [
    "# ANN Demo Model Evaluation with All Heads\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ANN Demo specific columns\n",
    "ANN_DEMO_X_COLS = ['LAP', 'MMT', 'CMC', 'CNF', 'SLK', 'AGR',\n",
    "                   'ALG', 'CAR', 'CHS', 'PEC', 'PUL', 'STA', 'GEL', 'GLU', 'ZIN', 'GLY',\n",
    "                   'FFA', 'LAC', 'LEV', 'PHA', 'SRB', 'SUA', 'XYL']\n",
    "\n",
    "ANN_DEMO_Y_COLS = ['TensileStress', 'TensileStrain','TensileModulusLog10', 'TensileToughnessMean100n90',\n",
    "                   'TransVis', 'TransIR', 'TransUV']\n",
    "\n",
    "# Load ANN demo data\n",
    "ann_demo_df = pd.read_csv('data/ann_demo.csv')\n",
    "print(f\"ANN Demo dataset shape: {ann_demo_df.shape}\")\n",
    "\n",
    "# Create train/test split using the same method as train_test_split.py\n",
    "X_train_ann, X_test_ann, y_train_ann, y_test_ann = train_test_split(\n",
    "    ann_demo_df[ANN_DEMO_X_COLS], \n",
    "    ann_demo_df[ANN_DEMO_Y_COLS], \n",
    "    test_size=0.2, \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train_ann.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test_ann.shape[0]} samples\")\n",
    "print(f\"Target columns: {ANN_DEMO_Y_COLS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optical Data Statistics (columns 4:7 are TransVis, TransIR, TransUV):\n",
      "======================================================================\n",
      "\n",
      "Train Optical Data:\n",
      "  Min values: [1.29561446e-03 2.36666667e+00 3.10000000e+00 0.00000000e+00]\n",
      "  Max values: [  9.98133898 275.1         85.43333333 100.        ]\n",
      "  Mean values: [ 0.88493767 70.12092796 69.22490842 45.13296703]\n",
      "  Std values: [ 1.41265209 17.46409791 11.69908822 14.12777834]\n",
      "  Values <= 0.01: [12  0  0  1]\n",
      "  Values == 0: [0 0 0 1]\n",
      "\n",
      "Test Optical Data:\n",
      "  Min values: [2.88704121e-03 4.65666667e+01 4.26000000e+01 1.90333333e+01]\n",
      "  Max values: [ 7.94972092 81.26666667 86.26666667 59.93333333]\n",
      "  Mean values: [ 0.77583781 70.3615942  69.14299517 44.57463768]\n",
      "  Std values: [ 1.26619188  8.27573491  9.32651252 11.16123294]\n",
      "  Values <= 0.01: [3 0 0 0]\n",
      "  Values == 0: [0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Diagnose optical data values to understand MRE issue\n",
    "print(\"Optical Data Statistics (columns 4:7 are TransVis, TransIR, TransUV):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Training data optical values\n",
    "train_optical = y_train_ann.iloc[:, 3:].values  # Last 3 columns\n",
    "print(\"\\nTrain Optical Data:\")\n",
    "print(f\"  Min values: {train_optical.min(axis=0)}\")\n",
    "print(f\"  Max values: {train_optical.max(axis=0)}\")\n",
    "print(f\"  Mean values: {train_optical.mean(axis=0)}\")\n",
    "print(f\"  Std values: {train_optical.std(axis=0)}\")\n",
    "print(f\"  Values <= 0.01: {(train_optical <= 0.01).sum(axis=0)}\")\n",
    "print(f\"  Values == 0: {(train_optical == 0).sum(axis=0)}\")\n",
    "\n",
    "# Test data optical values\n",
    "test_optical = y_test_ann.iloc[:, 3:].values\n",
    "print(\"\\nTest Optical Data:\")\n",
    "print(f\"  Min values: {test_optical.min(axis=0)}\")\n",
    "print(f\"  Max values: {test_optical.max(axis=0)}\")\n",
    "print(f\"  Mean values: {test_optical.mean(axis=0)}\")\n",
    "print(f\"  Std values: {test_optical.std(axis=0)}\")\n",
    "print(f\"  Values <= 0.01: {(test_optical <= 0.01).sum(axis=0)}\")\n",
    "print(f\"  Values == 0: {(test_optical == 0).sum(axis=0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN Demo Model Evaluation Functions\n",
    "def prepare_ann_demo_datasets(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Prepare ANN demo datasets for evaluation\n",
    "    \"\"\"\n",
    "    # Convert to float32\n",
    "    X_train_tensor = torch.tensor(X_train.values.astype(np.float32), dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test.values.astype(np.float32), dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values.astype(np.float32), dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.values.astype(np.float32), dtype=torch.float32)\n",
    "    \n",
    "    return {\n",
    "        'train_X': X_train_tensor,\n",
    "        'train_y': y_train_tensor,\n",
    "        'test_X': X_test_tensor,\n",
    "        'test_y': y_test_tensor\n",
    "    }\n",
    "\n",
    "def evaluate_ann_demo_model(model_tag, dataset, device='cpu'):\n",
    "    \"\"\"\n",
    "    Evaluate ANN demo model on all heads\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model, params = load_model_from_structure_with_txt(model_tag)\n",
    "    \n",
    "    if model is None:\n",
    "        return None\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Ensure model parameters are float32\n",
    "    for param in model.parameters():\n",
    "        param.data = param.data.float()\n",
    "    \n",
    "    results = {'model_tag': model_tag}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get predictions for all tasks\n",
    "        train_X = dataset['train_X'].to(device).float()\n",
    "        test_X = dataset['test_X'].to(device).float()\n",
    "        \n",
    "        # Evaluate each task head\n",
    "        for task in ['tensile', 'optical']:  # Based on the target columns\n",
    "            try:\n",
    "                # Get predictions\n",
    "                train_pred = model.forward(train_X, task).cpu().numpy()\n",
    "                test_pred = model.forward(test_X, task).cpu().numpy()\n",
    "                \n",
    "                # Get true values\n",
    "                if task == 'tensile':\n",
    "                    train_y = dataset['train_y'][:, :4].numpy()  # First 4 columns (tensile)\n",
    "                    test_y = dataset['test_y'][:, :4].numpy()\n",
    "                elif task == 'optical':\n",
    "                    train_y = dataset['train_y'][:, 4:7].numpy()  # Last 3 columns (optical)\n",
    "                    test_y = dataset['test_y'][:, 4:7].numpy()\n",
    "                \n",
    "                # Calculate MSE loss\n",
    "                train_mse = np.mean((train_y - train_pred) ** 2)\n",
    "                test_mse = np.mean((test_y - test_pred) ** 2)\n",
    "                \n",
    "                # Calculate MAE\n",
    "                train_mae = np.mean(np.abs(train_y - train_pred))\n",
    "                test_mae = np.mean(np.abs(test_y - test_pred))\n",
    "                \n",
    "                # Calculate R²\n",
    "                from sklearn.metrics import r2_score\n",
    "                train_r2 = r2_score(train_y, train_pred)\n",
    "                test_r2 = r2_score(test_y, test_pred)\n",
    "                \n",
    "                results[f'{task}_train_mse'] = train_mse\n",
    "                results[f'{task}_test_mse'] = test_mse\n",
    "                results[f'{task}_train_mae'] = train_mae\n",
    "                results[f'{task}_test_mae'] = test_mae\n",
    "                results[f'{task}_train_r2'] = train_r2\n",
    "                results[f'{task}_test_r2'] = test_r2\n",
    "                \n",
    "                # Calculate MRE (Mean Relative Error)\n",
    "                # Use threshold-based approach to avoid division by very small values\n",
    "                # Only calculate relative error for values above threshold\n",
    "                threshold = 0.1  # Ignore values below 0.1 for relative error calculation\n",
    "                \n",
    "                # Train MRE - only for values above threshold\n",
    "                train_mask = np.abs(train_y) > threshold\n",
    "                if train_mask.any():\n",
    "                    train_mre = np.mean(np.abs((train_y[train_mask] - train_pred[train_mask]) / train_y[train_mask]))\n",
    "                else:\n",
    "                    train_mre = np.nan\n",
    "                \n",
    "                # Test MRE - only for values above threshold\n",
    "                test_mask = np.abs(test_y) > threshold\n",
    "                if test_mask.any():\n",
    "                    test_mre = np.mean(np.abs((test_y[test_mask] - test_pred[test_mask]) / test_y[test_mask]))\n",
    "                else:\n",
    "                    test_mre = np.nan\n",
    "                \n",
    "                results[f'{task}_train_mre'] = train_mre\n",
    "                results[f'{task}_test_mre'] = test_mre\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating {task} for {model_tag}: {e}\")\n",
    "                results[f'{task}_train_mse'] = np.nan\n",
    "                results[f'{task}_test_mse'] = np.nan\n",
    "                results[f'{task}_train_mae'] = np.nan\n",
    "                results[f'{task}_test_mae'] = np.nan\n",
    "                results[f'{task}_train_r2'] = np.nan\n",
    "                results[f'{task}_test_r2'] = np.nan\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Prepare ANN demo datasets\n",
    "ann_demo_dataset = prepare_ann_demo_datasets(X_train_ann, X_test_ann, y_train_ann, y_test_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ANN Demo Model on All Available Models\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating models:  40%|████      | 2/5 [00:00<00:00, 14.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating pt_db.v3hp.RE5C on ANN demo data...\n",
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.RE5C\n",
      "  Tensile - Train MSE: 682.837402, Test MSE: 776.439636\n",
      "  Tensile - Train MRE: 0.896, Test MRE: 0.892\n",
      "  Tensile - Train R²: -3.535, Test R²: -2.350\n",
      "  Optical - Train MSE: 4047.579102, Test MSE: 3919.073486\n",
      "  Optical - Train MRE: 0.989, Test MRE: 0.990\n",
      "  Optical - Train R²: -20.021, Test R²: -46.753\n",
      "\n",
      "Evaluating pt_db.v3hp.2HNP on ANN demo data...\n",
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.2HNP\n",
      "  Tensile - Train MSE: 682.354370, Test MSE: 775.905701\n",
      "  Tensile - Train MRE: 0.895, Test MRE: 0.893\n",
      "  Tensile - Train R²: -3.477, Test R²: -2.321\n",
      "  Optical - Train MSE: 4048.314453, Test MSE: 3919.327393\n",
      "  Optical - Train MRE: 0.989, Test MRE: 0.990\n",
      "  Optical - Train R²: -20.024, Test R²: -46.755\n",
      "\n",
      "Evaluating pt_db.v3hp.CPUT on ANN demo data...\n",
      "{'model_params': {'encoder__hidden_layers': 50, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.CPUT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating models: 100%|██████████| 5/5 [00:00<00:00, 14.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tensile - Train MSE: 682.539978, Test MSE: 776.287231\n",
      "  Tensile - Train MRE: 0.888, Test MRE: 0.888\n",
      "  Tensile - Train R²: -3.482, Test R²: -2.350\n",
      "  Optical - Train MSE: 4048.046143, Test MSE: 3919.197510\n",
      "  Optical - Train MRE: 0.989, Test MRE: 0.990\n",
      "  Optical - Train R²: -20.023, Test R²: -46.754\n",
      "\n",
      "Evaluating pt_db.v3hp.VPLV on ANN demo data...\n",
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.VPLV\n",
      "  Tensile - Train MSE: 682.491211, Test MSE: 776.080933\n",
      "  Tensile - Train MRE: 0.890, Test MRE: 0.891\n",
      "  Tensile - Train R²: -3.454, Test R²: -2.336\n",
      "  Optical - Train MSE: 4048.774170, Test MSE: 3919.395508\n",
      "  Optical - Train MRE: 0.989, Test MRE: 0.990\n",
      "  Optical - Train R²: -20.026, Test R²: -46.757\n",
      "\n",
      "Evaluating pt_db.v3hp.PFFZ on ANN demo data...\n",
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 400.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.PFFZ\n",
      "  Tensile - Train MSE: 682.739136, Test MSE: 776.463684\n",
      "  Tensile - Train MRE: 0.889, Test MRE: 0.885\n",
      "  Tensile - Train R²: -3.504, Test R²: -2.379\n",
      "  Optical - Train MSE: 4048.752197, Test MSE: 3918.948975\n",
      "  Optical - Train MRE: 0.989, Test MRE: 0.990\n",
      "  Optical - Train R²: -20.027, Test R²: -46.750\n",
      "\n",
      "Successfully evaluated 5 models on ANN demo data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate ANN Demo Model on All Models\n",
    "print(\"Evaluating ANN Demo Model on All Available Models\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ann_demo_results = []\n",
    "\n",
    "for model_tag in tqdm(model_tags, desc=\"Evaluating models\"):\n",
    "    print(f\"\\nEvaluating {model_tag} on ANN demo data...\")\n",
    "    \n",
    "    result = evaluate_ann_demo_model(model_tag, ann_demo_dataset)\n",
    "    \n",
    "    if result is not None:\n",
    "        ann_demo_results.append(result)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"  Tensile - Train MSE: {result['tensile_train_mse']:.6f}, Test MSE: {result['tensile_test_mse']:.6f}\")\n",
    "        print(f\"  Tensile - Train MRE: {result['tensile_train_mre']:.3f}, Test MRE: {result['tensile_test_mre']:.3f}\")\n",
    "        print(f\"  Tensile - Train R²: {result['tensile_train_r2']:.3f}, Test R²: {result['tensile_test_r2']:.3f}\")\n",
    "        print(f\"  Optical - Train MSE: {result['optical_train_mse']:.6f}, Test MSE: {result['optical_test_mse']:.6f}\")\n",
    "        print(f\"  Optical - Train MRE: {result['optical_train_mre']:.3f}, Test MRE: {result['optical_test_mre']:.3f}\")\n",
    "        print(f\"  Optical - Train R²: {result['optical_train_r2']:.3f}, Test R²: {result['optical_test_r2']:.3f}\")\n",
    "    else:\n",
    "        print(f\"  ❌ Failed to evaluate {model_tag}\")\n",
    "\n",
    "print(f\"\\nSuccessfully evaluated {len(ann_demo_results)} models on ANN demo data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN DEMO MODEL EVALUATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "COMPREHENSIVE RESULTS:\n",
      "Model Tensile_Train_MSE Tensile_Test_MSE Tensile_Train_R2 Tensile_Test_R2 Optical_Train_MSE Optical_Test_MSE Optical_Train_R2 Optical_Test_R2 Tensile_Train_MRE Tensile_Test_MRE Optical_Train_MRE Optical_Test_MRE\n",
      " RE5C        682.837402       776.439636           -3.535          -2.350       4047.579102      3919.073486          -20.021         -46.753             0.896            0.892             0.989            0.990\n",
      " 2HNP        682.354370       775.905701           -3.477          -2.321       4048.314453      3919.327393          -20.024         -46.755             0.895            0.893             0.989            0.990\n",
      " CPUT        682.539978       776.287231           -3.482          -2.350       4048.046143      3919.197510          -20.023         -46.754             0.888            0.888             0.989            0.990\n",
      " VPLV        682.491211       776.080933           -3.454          -2.336       4048.774170      3919.395508          -20.026         -46.757             0.890            0.891             0.989            0.990\n",
      " PFFZ        682.739136       776.463684           -3.504          -2.379       4048.752197      3918.948975          -20.027         -46.750             0.889            0.885             0.989            0.990\n",
      "\n",
      "BEST MODELS:\n",
      "------------------------------\n",
      "Best Tensile Model: 2HNP\n",
      "  Test R²: -2.321\n",
      "  Test MSE: 775.905701\n",
      "  Test MRE: 0.893\n",
      "Best Optical Model: PFFZ\n",
      "  Test R²: -46.750\n",
      "  Test MSE: 3918.948975\n",
      "  Test MRE: 0.990\n"
     ]
    }
   ],
   "source": [
    "# ANN Demo Results Analysis\n",
    "if ann_demo_results:\n",
    "    ann_demo_df = pd.DataFrame(ann_demo_results)\n",
    "    \n",
    "    print(\"ANN DEMO MODEL EVALUATION RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create summary table\n",
    "    summary_data = []\n",
    "    for _, row in ann_demo_df.iterrows():\n",
    "        model_name = row['model_tag'].split('.')[-1]\n",
    "        summary_data.append({\n",
    "            'Model': model_name,\n",
    "            'Tensile_Train_MSE': f\"{row['tensile_train_mse']:.6f}\",\n",
    "            'Tensile_Test_MSE': f\"{row['tensile_test_mse']:.6f}\",\n",
    "            'Tensile_Train_R2': f\"{row['tensile_train_r2']:.3f}\",\n",
    "            'Tensile_Test_R2': f\"{row['tensile_test_r2']:.3f}\",\n",
    "            'Optical_Train_MSE': f\"{row['optical_train_mse']:.6f}\",\n",
    "            'Optical_Test_MSE': f\"{row['optical_test_mse']:.6f}\",\n",
    "            'Optical_Train_R2': f\"{row['optical_train_r2']:.3f}\",\n",
    "            'Optical_Test_R2': f\"{row['optical_test_r2']:.3f}\",\n",
    "            'Tensile_Train_MRE': f\"{row['tensile_train_mre']:.3f}\",\n",
    "            'Tensile_Test_MRE': f\"{row['tensile_test_mre']:.3f}\",\n",
    "            'Optical_Train_MRE': f\"{row['optical_train_mre']:.3f}\",\n",
    "            'Optical_Test_MRE': f\"{row['optical_test_mre']:.3f}\"\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(\"\\nCOMPREHENSIVE RESULTS:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Find best models\n",
    "    print(f\"\\nBEST MODELS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Best tensile model (by test R²)\n",
    "    best_tensile_idx = ann_demo_df['tensile_test_r2'].idxmax()\n",
    "    best_tensile = ann_demo_df.iloc[best_tensile_idx]\n",
    "    print(f\"Best Tensile Model: {best_tensile['model_tag'].split('.')[-1]}\")\n",
    "    print(f\"  Test R²: {best_tensile['tensile_test_r2']:.3f}\")\n",
    "    print(f\"  Test MSE: {best_tensile['tensile_test_mse']:.6f}\")\n",
    "    print(f\"  Test MRE: {best_tensile['tensile_test_mre']:.3f}\")\n",
    "    \n",
    "    # Best optical model (by test R²)\n",
    "    best_optical_idx = ann_demo_df['optical_test_r2'].idxmax()\n",
    "    best_optical = ann_demo_df.iloc[best_optical_idx]\n",
    "    print(f\"Best Optical Model: {best_optical['model_tag'].split('.')[-1]}\")\n",
    "    print(f\"  Test R²: {best_optical['optical_test_r2']:.3f}\")\n",
    "    print(f\"  Test MSE: {best_optical['optical_test_mse']:.6f}\")\n",
    "    print(f\"  Test MRE: {best_optical['optical_test_mre']:.3f}\")\n",
    "\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No ANN demo results to analyze!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 343 datapoints + UIP Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [01] Imports and Configuration Module Loaded\n",
      "📊 Dataset Configuration (aligned with ann_model):\n",
      "   📋 Input features: 23\n",
      "   🎯 Prediction tasks: ['optical', 'tensile']\n",
      "      optical: 3 targets - ['TransVis', 'TransIR', 'TransUV']\n",
      "      tensile: 4 targets - ['TensileStress', 'TensileStrain', 'TensileModulusLog10', 'TensileToughnessMean100n90']\n",
      "🔧 Environment Information:\n",
      "   🔮 PyTorch version: 2.8.0\n",
      "   💻 Device: CPU\n",
      "   🎲 Random seed: 0\n",
      "   🚀 Parallel processing: 10 cores available\n",
      "📁 Loaded dataset shape: (342, 42)\n",
      "🧹 After removing missing targets: (342, 42)\n",
      "📈 Data retention: 100.0%\n",
      "\n",
      "📋 Dataset Summary:\n",
      "   Total samples: 342\n",
      "   Input features: 23\n",
      "   Target variables: 7\n",
      "   Missing values in targets: 0\n",
      "\n",
      "📊 Target Variable Ranges:\n",
      "\n",
      "OPTICAL Targets:\n",
      "  TransVis: [2.37, 275.10] (mean: 70.17)\n",
      "  TransIR: [3.10, 86.27] (mean: 69.21)\n",
      "  TransUV: [0.00, 100.00] (mean: 45.02)\n",
      "\n",
      "TENSILE Targets:\n",
      "  TensileStress: [0.36, 161.67] (mean: 39.95)\n",
      "  TensileStrain: [0.07, 75.03] (mean: 5.84)\n",
      "  TensileModulusLog10: [0.60, 4.34] (mean: 3.50)\n",
      "  TensileToughnessMean100n90: [0.00, 9.98] (mean: 0.86)\n",
      "\n",
      "✅ [02] Data Loading and Preparation Module Loaded\n",
      "🗂️  Dataset Processing Summary:\n",
      "   📊 Original dataset: 342 samples\n",
      "   🧹 After cleaning: 342 samples\n",
      "   📈 Data retention: 100.0%\n",
      "   🔗 Input features: 23\n",
      "   🎯 Target variables: 7\n",
      "📊 Deep Dive - Target Statistics:\n",
      "\n",
      "OPTICAL Properties:\n",
      "  TransVis: [2.367, 275.100] μ=70.169, σ=16.064\n",
      "  TransIR: [3.100, 86.267] μ=69.208, σ=11.277\n",
      "  TransUV: [0.000, 100.000] μ=45.020, σ=13.603\n",
      "\n",
      "TENSILE Properties:\n",
      "  TensileStress: [0.365, 161.667] μ=39.950, σ=32.949\n",
      "  TensileStrain: [0.070, 75.035] μ=5.838, σ=11.199\n",
      "  TensileModulusLog10: [0.596, 4.342] μ=3.501, σ=0.717\n",
      "  TensileToughnessMean100n90: [0.001, 9.981] μ=0.863, σ=1.387\n",
      "✅ Neural Network Architecture Created (aligned with ann_model/base.py)\n",
      "📏 Input dimension: 23\n",
      "🔗 Hidden output dimension: 64\n",
      "⚙️ Total parameters: 11,328\n",
      "💾 Model size: ~44.2 KB\n",
      "\n",
      "✅ [03] Neural Network Architecture Module Loaded\n",
      "🏗️ Architecture Summary (ANNModel from ann_model/base.py):\n",
      "   📏 Input features: 23\n",
      "   🔗 Output dimensions: 64\n",
      "   ⚙️ Total parameters: 11,328\n",
      "   💾 Memory footprint: ~44.2 KB\n",
      "🔧 Key Features:\n",
      "   🏗️ ModuleDict-based layers: ✅\n",
      "   🎲 Dropout support: ✅\n",
      "   🎯 Weight initialization: kaiming_uniform_\n",
      "   🔄 All PyTorch activations supported\n",
      "⚠️ smogn not available, SMOTE will use fallback method\n",
      "✅ Section 4: Data Augmentation Methods Complete\n",
      "🔧 UIP parameters (base): comp_sigma=0.01, prop_sigma=0.15\n",
      "🔧 UIP uses HYBRID noise approach:\n",
      "   • Experimental std (when available) - most realistic!\n",
      "   • Adaptive relative noise (fallback) - scales with sqrt(ratio)\n",
      "   • ratio=1: 15% fallback, ratio=10: 4.7% fallback, ratio=100: 1.5% fallback\n",
      "🔧 SMOTE-Regression: Using fallback (smogn not available)\n",
      "   • Install smogn for optimal SMOTE performance: pip install smogn\n",
      "🔧 Ratio-based functions: UIP and SMOTE ratio variants available\n",
      "✅ Section 5: Training Utilities Complete (aligned with ann_model/trainer.py)\n",
      "🔧 Hyperparameter space: 3072 total combinations\n",
      "🔧 Training function: train_model() with Adam optimizer\n",
      "🔧 Metrics: compute_mre_percent() aligned with ann_model/metrics.py\n",
      "🔧 Ensemble function: Full pipeline with error handling\n",
      "✅ [06] Model Architectures and Hyperparameter Configurations Loaded\n",
      "   (Aligned with ann_model/config.py)\n",
      "📊 Config Classes:\n",
      "   ✓ ModelConfig: Encoder + task-specific heads (grade, tensile, optical, fire)\n",
      "   ✓ FitConfig: Training hyperparameters\n",
      "📊 Architecture configurations: 3\n",
      "🔧 Hyperparameter sets per architecture: 5\n",
      "🎯 SCALED Architecture Strategy:\n",
      "   ✓ Simple (1:1):   (64, 32) - batch=32, epochs=100, dropout=0.1\n",
      "   ✓ Medium (1:10):  (128, 64, 32) - batch=64, epochs=150, dropout=0.15 + BatchNorm\n",
      "   ✓ Complex (1:100): (256, 128, 64, 32) - batch=128, epochs=200, dropout=0.2 + BatchNorm\n",
      "   ✓ More data → Larger models → Better capacity without overfitting\n",
      "✅ [07] Parallel Training Utilities Loaded\n",
      "   (Aligned with ann_model/metrics.py and ann_model/trainer.py)\n",
      "🚀 Parallel processing configured for ensemble training\n",
      "🔧 Functions: train_single_model_parallel(), train_ensemble_parallel()\n",
      "📊 Metrics: MSE, MAE, MRE (%), R² - matching ann_model/metrics.py\n",
      "✅ [08] Experiment Runners Loaded (aligned with ann_model, with data-driven scales)\n",
      "🔧 Functions: run_uip_experiment(), run_smote_experiment(), run_baseline_experiment()\n",
      "🎯 Each function runs full ensemble experiments for a specific augmentation method\n",
      "⚠️  Key Update: Targets are scaled using data-driven TARGET_Y_SCALES:\n",
      "   • Optical: Scaled by [300, 100, 100] for TransVis/TransIR/TransUV (Sigmoid activation)\n",
      "   • Tensile: Scaled by [150, 150, 3, 20] (ReLU activation)\n",
      "   • MRE and R² calculated on ORIGINAL scale for interpretability\n",
      "✅ Core sections loaded.\n"
     ]
    }
   ],
   "source": [
    "for sec in [\n",
    "    '01_imports_and_configuration',\n",
    "    '02_data_loading_and_preparation',\n",
    "    '03_neural_network_architecture',\n",
    "    '04_data_augmentation_methods',\n",
    "    '05_training_and_hyperparameter_tuning',\n",
    "    '06_model_architectures_and_configs',\n",
    "    '07_parallel_training_utilities',\n",
    "    '08_experiment_runners'\n",
    "]:\n",
    "    exec(open(f'core_sections/{sec}.py').read())\n",
    "print(\"✅ Core sections loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UIP on Train + Evaluate Loaded Models (tensile, optical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying UIP with ratio 1:100 on train (optical & tensile). Aug samples: 27573 / 273x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating models on test set (tensile+optical):  20%|██        | 1/5 [00:00<00:00,  8.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.RE5C\n",
      "{'model_params': {'encoder__hidden_layers': 30, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 1, 'tensile__hidden_layers': 1, 'optical__hidden_layers': 1, 'fire__hidden_layers': 1, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.2HNP\n",
      "{'model_params': {'encoder__hidden_layers': 50, 'encoder__hidden_base': 320.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.CPUT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating models on test set (tensile+optical): 100%|██████████| 5/5 [00:00<00:00, 14.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 360.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.VPLV\n",
      "{'model_params': {'encoder__hidden_layers': 40, 'encoder__hidden_base': 400.0, 'encoder__hidden_scale': 'log_1.025', 'encoder__n_output': 128, 'grade__hidden_layers': 2, 'tensile__hidden_layers': 2, 'optical__hidden_layers': 2, 'fire__hidden_layers': 2, 'dtype_str': 'float32'}}\n",
      "Successfully loaded model: pt_db.v3hp.PFFZ\n",
      "\n",
      "UIP-based evaluation (train augmented, test original) summary (per model):\n",
      "      model_tag  uip_ratio  tensile_test_r2  tensile_test_mre  tensile_test_mse  optical_test_r2  optical_test_mre  optical_test_mse\n",
      "pt_db.v3hp.RE5C        100         0.197747          0.926468        107.968506      -101.433983          0.728919       7021.876953\n",
      "pt_db.v3hp.2HNP        100         0.239848          0.918564        128.935425      -101.303535          0.722915       7011.377441\n",
      "pt_db.v3hp.CPUT        100         0.088682          1.028252         92.967514      -100.033073          0.734504       6929.611328\n",
      "pt_db.v3hp.VPLV        100         0.146410          1.164428        117.128990       -99.245689          0.733691       6874.820312\n",
      "pt_db.v3hp.PFFZ        100         0.016572          1.144332         94.102684      -102.988655          0.739769       7130.172363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply UIP on ANN demo train split and evaluate loaded models\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Ensure core sections (including apply_uip_augmentation) are loaded\n",
    "try:\n",
    "    _ = apply_uip_augmentation  # noqa: F401\n",
    "except NameError:\n",
    "    for sec in [\n",
    "        '01_imports_and_configuration',\n",
    "        '02_data_loading_and_preparation',\n",
    "        '03_neural_network_architecture',\n",
    "        '04_data_augmentation_methods',\n",
    "        '05_training_and_hyperparameter_tuning',\n",
    "        '06_model_architectures_and_configs',\n",
    "        '07_parallel_training_utilities',\n",
    "        '08_experiment_runners'\n",
    "    ]:\n",
    "        exec(open(f'core_sections/{sec}.py').read())\n",
    "\n",
    "# Prepare ANN demo train/test arrays\n",
    "X_train = X_train_ann[ANN_DEMO_X_COLS].values.astype(np.float32)\n",
    "X_test = X_test_ann[ANN_DEMO_X_COLS].values.astype(np.float32)\n",
    "\n",
    "y_train_full = y_train_ann[ANN_DEMO_Y_COLS].values.astype(np.float32)\n",
    "y_test_full = y_test_ann[ANN_DEMO_Y_COLS].values.astype(np.float32)\n",
    "\n",
    "# Split targets into tasks\n",
    "optical_cols = TARGET_Y_COLS['optical']\n",
    "tensile_cols = TARGET_Y_COLS['tensile']\n",
    "\n",
    "Y_train_opt = y_train_ann[optical_cols].values.astype(np.float32)\n",
    "Y_test_opt = y_test_ann[optical_cols].values.astype(np.float32)\n",
    "\n",
    "Y_train_ten = y_train_ann[tensile_cols].values.astype(np.float32)\n",
    "Y_test_ten = y_test_ann[tensile_cols].values.astype(np.float32)\n",
    "\n",
    "# Optional: experimental stds not available here\n",
    "Y_train_opt_std = None\n",
    "Y_train_ten_std = None\n",
    "\n",
    "# UIP augmentation on raw train for both tasks\n",
    "ratio = 100\n",
    "X_aug_opt, Y_aug_opt = apply_uip_augmentation(X_train, Y_train_opt, ratio, 'optical', y_train_std=Y_train_opt_std)\n",
    "X_aug_ten, Y_aug_ten = apply_uip_augmentation(X_train, Y_train_ten, ratio, 'tensile', y_train_std=Y_train_ten_std)\n",
    "\n",
    "# For evaluation of loaded models, we only need feature scalers (models expect raw inputs); keep X as raw tensors later\n",
    "# Build scalers for potential normalization use (not applied to model inputs here)\n",
    "feature_scaler = StandardScaler().fit(np.vstack([X_aug_opt, X_aug_ten]))\n",
    "\n",
    "# Target scaling per core sections\n",
    "optical_scales = np.array(TARGET_Y_SCALES['optical'], dtype=np.float32)\n",
    "tensile_scales = np.array(TARGET_Y_SCALES['tensile'], dtype=np.float32)\n",
    "\n",
    "# Function to evaluate one model on both heads\n",
    "def eval_model_on_heads(model_tag: str):\n",
    "    model, params = load_model_from_structure_with_txt(model_tag)\n",
    "    if model is None:\n",
    "        return None\n",
    "    model.eval()\n",
    "    # Ensure float32\n",
    "    for p in model.parameters():\n",
    "        p.data = p.data.float()\n",
    "    with torch.no_grad():\n",
    "        Xtr_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        Xte_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "        # Tensile\n",
    "        ten_train_pred = model.forward(Xtr_tensor, 'tensile').cpu().numpy()\n",
    "        ten_test_pred = model.forward(Xte_tensor, 'tensile').cpu().numpy()\n",
    "        # Inverse scale predictions and truths to original scale\n",
    "        ten_train_pred_orig = ten_train_pred * tensile_scales\n",
    "        ten_test_pred_orig = ten_test_pred * tensile_scales\n",
    "        ten_train_true_orig = Y_train_ten\n",
    "        ten_test_true_orig = Y_test_ten\n",
    "        # Metrics (original scale)\n",
    "        ten_train_mse = mean_squared_error(ten_train_true_orig, ten_train_pred_orig)\n",
    "        ten_test_mse = mean_squared_error(ten_test_true_orig, ten_test_pred_orig)\n",
    "        ten_train_mae = mean_absolute_error(ten_train_true_orig, ten_train_pred_orig)\n",
    "        ten_test_mae = mean_absolute_error(ten_test_true_orig, ten_test_pred_orig)\n",
    "        ten_train_r2 = r2_score(ten_train_true_orig, ten_train_pred_orig)\n",
    "        ten_test_r2 = r2_score(ten_test_true_orig, ten_test_pred_orig)\n",
    "        # MRE with threshold\n",
    "        threshold = 0.1\n",
    "        mask_tr = np.abs(ten_train_true_orig) > threshold\n",
    "        mask_te = np.abs(ten_test_true_orig) > threshold\n",
    "        ten_train_mre = np.mean(np.abs((ten_train_true_orig[mask_tr] - ten_train_pred_orig[mask_tr]) / ten_train_true_orig[mask_tr])) if mask_tr.any() else np.nan\n",
    "        ten_test_mre = np.mean(np.abs((ten_test_true_orig[mask_te] - ten_test_pred_orig[mask_te]) / ten_test_true_orig[mask_te])) if mask_te.any() else np.nan\n",
    "        # Optical\n",
    "        opt_train_pred = model.forward(Xtr_tensor, 'optical').cpu().numpy()\n",
    "        opt_test_pred = model.forward(Xte_tensor, 'optical').cpu().numpy()\n",
    "        opt_train_pred_orig = opt_train_pred * optical_scales\n",
    "        opt_test_pred_orig = opt_test_pred * optical_scales\n",
    "        opt_train_true_orig = Y_train_opt\n",
    "        opt_test_true_orig = Y_test_opt\n",
    "        opt_train_mse = mean_squared_error(opt_train_true_orig, opt_train_pred_orig)\n",
    "        opt_test_mse = mean_squared_error(opt_test_true_orig, opt_test_pred_orig)\n",
    "        opt_train_mae = mean_absolute_error(opt_train_true_orig, opt_train_pred_orig)\n",
    "        opt_test_mae = mean_absolute_error(opt_test_true_orig, opt_test_pred_orig)\n",
    "        opt_train_r2 = r2_score(opt_train_true_orig, opt_train_pred_orig)\n",
    "        opt_test_r2 = r2_score(opt_test_true_orig, opt_test_pred_orig)\n",
    "        mask_tr_o = np.abs(opt_train_true_orig) > threshold\n",
    "        mask_te_o = np.abs(opt_test_true_orig) > threshold\n",
    "        opt_train_mre = np.mean(np.abs((opt_train_true_orig[mask_tr_o] - opt_train_pred_orig[mask_tr_o]) / opt_train_true_orig[mask_tr_o])) if mask_tr_o.any() else np.nan\n",
    "        opt_test_mre = np.mean(np.abs((opt_test_true_orig[mask_te_o] - opt_test_pred_orig[mask_te_o]) / opt_test_true_orig[mask_te_o])) if mask_te_o.any() else np.nan\n",
    "    return {\n",
    "        'model_tag': model_tag,\n",
    "        'tensile_train_mse': ten_train_mse,\n",
    "        'tensile_test_mse': ten_test_mse,\n",
    "        'tensile_train_mae': ten_train_mae,\n",
    "        'tensile_test_mae': ten_test_mae,\n",
    "        'tensile_train_r2': ten_train_r2,\n",
    "        'tensile_test_r2': ten_test_r2,\n",
    "        'tensile_train_mre': ten_train_mre,\n",
    "        'tensile_test_mre': ten_test_mre,\n",
    "        'optical_train_mse': opt_train_mse,\n",
    "        'optical_test_mse': opt_test_mse,\n",
    "        'optical_train_mae': opt_train_mae,\n",
    "        'optical_test_mae': opt_test_mae,\n",
    "        'optical_train_r2': opt_train_r2,\n",
    "        'optical_test_r2': opt_test_r2,\n",
    "        'optical_train_mre': opt_train_mre,\n",
    "        'optical_test_mre': opt_test_mre,\n",
    "        'uip_ratio': ratio,\n",
    "        'uip_train_samples': int(X_aug_opt.shape[0])  # same count for ten/opt start size\n",
    "    }\n",
    "\n",
    "print(f\"Applying UIP with ratio 1:{ratio} on train (optical & tensile). Aug samples: {X_aug_opt.shape[0]} / {len(X_train)}x\")\n",
    "\n",
    "# Evaluate all loaded models\n",
    "uip_eval_results = []\n",
    "for tag in tqdm(model_tags, desc=\"Evaluating models on test set (tensile+optical)\"):\n",
    "    res = eval_model_on_heads(tag)\n",
    "    if res is not None:\n",
    "        uip_eval_results.append(res)\n",
    "\n",
    "uip_eval_df = pd.DataFrame(uip_eval_results)\n",
    "print(\"\\nUIP-based evaluation (train augmented, test original) summary (per model):\")\n",
    "if not uip_eval_df.empty:\n",
    "    display_cols = [\n",
    "        'model_tag', 'uip_ratio',\n",
    "        'tensile_test_r2', 'tensile_test_mre', 'tensile_test_mse',\n",
    "        'optical_test_r2', 'optical_test_mre', 'optical_test_mse'\n",
    "    ]\n",
    "    print(uip_eval_df[display_cols].to_string(index=False))\n",
    "\n",
    "else:\n",
    "    print(\"No results to display.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
